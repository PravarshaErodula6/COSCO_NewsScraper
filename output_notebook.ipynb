{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17d3cffa",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [5]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "912da723",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T03:38:27.506168Z",
     "iopub.status.busy": "2025-04-09T03:38:27.505880Z",
     "iopub.status.idle": "2025-04-09T03:38:27.922599Z",
     "shell.execute_reply": "2025-04-09T03:38:27.922294Z"
    },
    "papermill": {
     "duration": 0.431713,
     "end_time": "2025-04-09T03:38:27.923347",
     "exception": false,
     "start_time": "2025-04-09T03:38:27.491634",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- --------------\r\n",
      "ansicolors                1.1.8\r\n",
      "anyio                     4.8.0\r\n",
      "appnope                   0.1.4\r\n",
      "argon2-cffi               23.1.0\r\n",
      "argon2-cffi-bindings      21.2.0\r\n",
      "arrow                     1.3.0\r\n",
      "asttokens                 3.0.0\r\n",
      "async-lru                 2.0.4\r\n",
      "attrs                     25.1.0\r\n",
      "babel                     2.17.0\r\n",
      "beautifulsoup4            4.13.3\r\n",
      "bleach                    6.2.0\r\n",
      "certifi                   2025.1.31\r\n",
      "cffi                      1.17.1\r\n",
      "charset-normalizer        3.4.1\r\n",
      "click                     8.1.8\r\n",
      "comm                      0.2.2\r\n",
      "debugpy                   1.8.12\r\n",
      "decorator                 5.1.1\r\n",
      "defusedxml                0.7.1\r\n",
      "entrypoints               0.4\r\n",
      "executing                 2.2.0\r\n",
      "fastjsonschema            2.21.1\r\n",
      "feedparser                6.0.11\r\n",
      "filelock                  3.17.0\r\n",
      "fqdn                      1.5.1\r\n",
      "fsspec                    2025.2.0\r\n",
      "h11                       0.14.0\r\n",
      "httpcore                  1.0.7\r\n",
      "httpx                     0.28.1\r\n",
      "huggingface-hub           0.28.1\r\n",
      "idna                      3.10\r\n",
      "ipykernel                 6.29.5\r\n",
      "ipython                   8.32.0\r\n",
      "ipywidgets                8.1.5\r\n",
      "isoduration               20.11.0\r\n",
      "jedi                      0.19.2\r\n",
      "Jinja2                    3.1.5\r\n",
      "json5                     0.10.0\r\n",
      "jsonpointer               3.0.0\r\n",
      "jsonschema                4.23.0\r\n",
      "jsonschema-specifications 2024.10.1\r\n",
      "jupyter                   1.1.1\r\n",
      "jupyter_client            8.6.3\r\n",
      "jupyter-console           6.6.3\r\n",
      "jupyter_core              5.7.2\r\n",
      "jupyter-events            0.12.0\r\n",
      "jupyter-lsp               2.2.5\r\n",
      "jupyter_server            2.15.0\r\n",
      "jupyter_server_terminals  0.5.3\r\n",
      "jupyterlab                4.3.5\r\n",
      "jupyterlab_pygments       0.3.0\r\n",
      "jupyterlab_server         2.27.3\r\n",
      "jupyterlab_widgets        3.0.13\r\n",
      "MarkupSafe                3.0.2\r\n",
      "matplotlib-inline         0.1.7\r\n",
      "mistune                   3.1.1\r\n",
      "mpmath                    1.3.0\r\n",
      "nbclient                  0.10.2\r\n",
      "nbconvert                 7.16.6\r\n",
      "nbformat                  5.10.4\r\n",
      "nest-asyncio              1.6.0\r\n",
      "networkx                  3.4.2\r\n",
      "notebook                  7.3.2\r\n",
      "notebook_shim             0.2.4\r\n",
      "numpy                     2.2.2\r\n",
      "outcome                   1.3.0.post0\r\n",
      "overrides                 7.7.0\r\n",
      "packaging                 24.2\r\n",
      "pandas                    2.2.3\r\n",
      "pandocfilters             1.5.1\r\n",
      "papermill                 2.6.0\r\n",
      "parso                     0.8.4\r\n",
      "pexpect                   4.9.0\r\n",
      "pillow                    11.1.0\r\n",
      "pip                       25.0.1\r\n",
      "platformdirs              4.3.6\r\n",
      "prometheus_client         0.21.1\r\n",
      "prompt_toolkit            3.0.50\r\n",
      "psutil                    6.1.1\r\n",
      "ptyprocess                0.7.0\r\n",
      "pure_eval                 0.2.3\r\n",
      "pycparser                 2.22\r\n",
      "Pygments                  2.19.1\r\n",
      "PySocks                   1.7.1\r\n",
      "python-dateutil           2.9.0.post0\r\n",
      "python-dotenv             1.0.1\r\n",
      "python-json-logger        3.2.1\r\n",
      "pytz                      2025.1\r\n",
      "PyYAML                    6.0.2\r\n",
      "pyzmq                     26.2.1\r\n",
      "referencing               0.36.2\r\n",
      "regex                     2024.11.6\r\n",
      "requests                  2.32.3\r\n",
      "rfc3339-validator         0.1.4\r\n",
      "rfc3986-validator         0.1.1\r\n",
      "rpds-py                   0.22.3\r\n",
      "safetensors               0.5.2\r\n",
      "scapy                     2.6.1\r\n",
      "selenium                  4.28.1\r\n",
      "Send2Trash                1.8.3\r\n",
      "setuptools                75.8.0\r\n",
      "sgmllib3k                 1.0.0\r\n",
      "six                       1.17.0\r\n",
      "sniffio                   1.3.1\r\n",
      "sortedcontainers          2.4.0\r\n",
      "soupsieve                 2.6\r\n",
      "stack-data                0.6.3\r\n",
      "sympy                     1.13.1\r\n",
      "tenacity                  9.1.2\r\n",
      "terminado                 0.18.1\r\n",
      "tinycss2                  1.4.0\r\n",
      "tokenizers                0.21.0\r\n",
      "torch                     2.6.0\r\n",
      "torchaudio                2.6.0\r\n",
      "torchvision               0.21.0\r\n",
      "tornado                   6.4.2\r\n",
      "tqdm                      4.67.1\r\n",
      "traitlets                 5.14.3\r\n",
      "transformers              4.49.0\r\n",
      "trio                      0.28.0\r\n",
      "trio-websocket            0.11.1\r\n",
      "types-python-dateutil     2.9.0.20241206\r\n",
      "typing_extensions         4.12.2\r\n",
      "tzdata                    2025.1\r\n",
      "undetected-chromedriver   3.5.5\r\n",
      "uri-template              1.3.0\r\n",
      "urllib3                   2.3.0\r\n",
      "wcwidth                   0.2.13\r\n",
      "webcolors                 24.11.1\r\n",
      "webdriver-manager         4.0.2\r\n",
      "webencodings              0.5.1\r\n",
      "websocket-client          1.8.0\r\n",
      "websockets                15.0.1\r\n",
      "widgetsnbextension        4.0.13\r\n",
      "wsproto                   1.2.0\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0993acac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T03:38:27.932666Z",
     "iopub.status.busy": "2025-04-09T03:38:27.932539Z",
     "iopub.status.idle": "2025-04-09T03:38:28.489644Z",
     "shell.execute_reply": "2025-04-09T03:38:28.489361Z"
    },
    "papermill": {
     "duration": 0.561632,
     "end_time": "2025-04-09T03:38:28.490408",
     "exception": false,
     "start_time": "2025-04-09T03:38:27.928776",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (4.28.1)\r\n",
      "Requirement already satisfied: webdriver-manager in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (4.0.2)\r\n",
      "Requirement already satisfied: pandas in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (2.2.3)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: urllib3<3,>=1.26 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.3.0)\r\n",
      "Requirement already satisfied: trio~=0.17 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from selenium) (0.28.0)\r\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from selenium) (0.11.1)\r\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from selenium) (2025.1.31)\r\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from selenium) (4.12.2)\r\n",
      "Requirement already satisfied: websocket-client~=1.8 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from selenium) (1.8.0)\r\n",
      "Requirement already satisfied: requests in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from webdriver-manager) (2.32.3)\r\n",
      "Requirement already satisfied: python-dotenv in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from webdriver-manager) (1.0.1)\r\n",
      "Requirement already satisfied: packaging in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from webdriver-manager) (24.2)\r\n",
      "Requirement already satisfied: numpy>=1.26.0 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from pandas) (2.2.2)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from pandas) (2025.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\r\n",
      "Requirement already satisfied: attrs>=23.2.0 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from trio~=0.17->selenium) (25.1.0)\r\n",
      "Requirement already satisfied: sortedcontainers in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from trio~=0.17->selenium) (2.4.0)\r\n",
      "Requirement already satisfied: idna in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from trio~=0.17->selenium) (3.10)\r\n",
      "Requirement already satisfied: outcome in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\r\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from trio~=0.17->selenium) (1.3.1)\r\n",
      "Requirement already satisfied: wsproto>=0.14 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\r\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from requests->webdriver-manager) (3.4.1)\r\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /Users/pravarshaerodula/myenv/lib/python3.13/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install selenium webdriver-manager pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c06f7cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T03:38:28.499117Z",
     "iopub.status.busy": "2025-04-09T03:38:28.498993Z",
     "iopub.status.idle": "2025-04-09T03:38:28.900318Z",
     "shell.execute_reply": "2025-04-09T03:38:28.900097Z"
    },
    "papermill": {
     "duration": 0.406445,
     "end_time": "2025-04-09T03:38:28.901029",
     "exception": false,
     "start_time": "2025-04-09T03:38:28.494584",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selenium and required packages are installed successfully!\n"
     ]
    }
   ],
   "source": [
    "import selenium\n",
    "import pandas as pd\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "print(\"Selenium and required packages are installed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ce38dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T03:38:28.909348Z",
     "iopub.status.busy": "2025-04-09T03:38:28.909171Z",
     "iopub.status.idle": "2025-04-09T03:38:34.746958Z",
     "shell.execute_reply": "2025-04-09T03:38:34.746516Z"
    },
    "papermill": {
     "duration": 5.843442,
     "end_time": "2025-04-09T03:38:34.748609",
     "exception": false,
     "start_time": "2025-04-09T03:38:28.905167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Website loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# Automatically find and install the correct ChromeDriver version\n",
    "chrome_driver_path = ChromeDriverManager().install()\n",
    "\n",
    "# Setup Chrome options\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run without GUI\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Initialize WebDriver\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# Open a test website\n",
    "url = \"https://www.upstreamonline.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "print(\"Website loaded successfully!\")\n",
    "\n",
    "# Close the driver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53d9a88",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07ed1a44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T03:38:34.757797Z",
     "iopub.status.busy": "2025-04-09T03:38:34.757676Z",
     "iopub.status.idle": "2025-04-09T03:38:35.118425Z",
     "shell.execute_reply": "2025-04-09T03:38:35.117903Z"
    },
    "papermill": {
     "duration": 0.365597,
     "end_time": "2025-04-09T03:38:35.119178",
     "exception": true,
     "start_time": "2025-04-09T03:38:34.753581",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "MaxRetryError",
     "evalue": "HTTPConnectionPool(host='localhost', port=64805): Max retries exceeded with url: /session/2c6eee54bcaf9bc2a029970ef50af76c/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1075f48a0>: Failed to establish a new connection: [Errno 61] Connection refused'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connection.py:198\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 198\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 787\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py:493\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 493\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    505\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connection.py:445\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 445\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1333\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1333\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1093\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1092\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1093\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1096\u001b[0m \n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/http/client.py:1037\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1036\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m-> 1037\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1038\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connection.py:276\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    278\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connection.py:213\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m NewConnectionError(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;28mself\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to establish a new connection: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    215\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    217\u001b[0m sys\u001b[38;5;241m.\u001b[39maudit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp.client.connect\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mport)\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x1075f48a0>: Failed to establish a new connection: [Errno 61] Connection refused",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mselenium\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mwebdriver\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msupport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m expected_conditions \u001b[38;5;28;01mas\u001b[39;00m EC\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Wait for the page to load fully\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mWebDriverWait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muntil\u001b[49m\u001b[43m(\u001b[49m\u001b[43mEC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpresence_of_element_located\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTAG_NAME\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Find all article links\u001b[39;00m\n\u001b[1;32m      9\u001b[0m articles \u001b[38;5;241m=\u001b[39m driver\u001b[38;5;241m.\u001b[39mfind_elements(By\u001b[38;5;241m.\u001b[39mTAG_NAME, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/selenium/webdriver/support/wait.py:137\u001b[0m, in \u001b[0;36mWebDriverWait.until\u001b[0;34m(self, method, message)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_driver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m value:\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/selenium/webdriver/support/expected_conditions.py:110\u001b[0m, in \u001b[0;36mpresence_of_element_located.<locals>._predicate\u001b[0;34m(driver)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_predicate\u001b[39m(driver: WebDriverOrWebElement):\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdriver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_element\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlocator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:888\u001b[0m, in \u001b[0;36mWebDriver.find_element\u001b[0;34m(self, by, value)\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoSuchElementException(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot locate relative element with: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mby\u001b[38;5;241m.\u001b[39mroot\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    886\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m elements[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m--> 888\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFIND_ELEMENT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musing\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/selenium/webdriver/remote/webdriver.py:427\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[1;32m    425\u001b[0m         params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msessionId\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msession_id\n\u001b[0;32m--> 427\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommand_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdriver_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merror_handler\u001b[38;5;241m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/selenium/webdriver/remote/remote_connection.py:404\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    402\u001b[0m trimmed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trim_large_entries(params)\n\u001b[1;32m    403\u001b[0m LOGGER\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, command_info[\u001b[38;5;241m0\u001b[39m], url, \u001b[38;5;28mstr\u001b[39m(trimmed))\n\u001b[0;32m--> 404\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand_info\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/selenium/webdriver/remote/remote_connection.py:428\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    425\u001b[0m     body \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client_config\u001b[38;5;241m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 428\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client_config\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    429\u001b[0m     statuscode \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mstatus\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/_request_methods.py:143\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    136\u001b[0m         method,\n\u001b[1;32m    137\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39murlopen_kw,\n\u001b[1;32m    141\u001b[0m     )\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_encode_body\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfields\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfields\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43murlopen_kw\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/_request_methods.py:278\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    274\u001b[0m     extra_kw[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mContent-Type\u001b[39m\u001b[38;5;124m\"\u001b[39m, content_type)\n\u001b[1;32m    276\u001b[0m extra_kw\u001b[38;5;241m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 278\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py:871\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    868\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    870\u001b[0m     )\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    890\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py:871\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    868\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    870\u001b[0m     )\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    890\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py:871\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    867\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    868\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    869\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    870\u001b[0m     )\n\u001b[0;32m--> 871\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    890\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/connectionpool.py:841\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    838\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_e, (\u001b[38;5;167;01mOSError\u001b[39;00m, HTTPException)):\n\u001b[1;32m    839\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 841\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    844\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n\u001b[1;32m    846\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[0;32m~/myenv/lib/python3.13/site-packages/urllib3/util/retry.py:519\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m    518\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 519\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    521\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n\u001b[1;32m    523\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_retry\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='localhost', port=64805): Max retries exceeded with url: /session/2c6eee54bcaf9bc2a029970ef50af76c/element (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x1075f48a0>: Failed to establish a new connection: [Errno 61] Connection refused'))"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Wait for the page to load fully\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"a\")))\n",
    "\n",
    "# Find all article links\n",
    "articles = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "# Extract links that contain keywords\n",
    "keywords = [\"fid\", \"rig\"]\n",
    "filtered_links = []\n",
    "\n",
    "for article in articles:\n",
    "    try:\n",
    "        link = article.get_attribute(\"href\")\n",
    "        if link and link.startswith(\"https\") and any(keyword in link.lower() for keyword in keywords):\n",
    "            filtered_links.append(link)\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting link: {e}\")\n",
    "\n",
    "print(f\"Found {len(filtered_links)} relevant articles:\")\n",
    "for link in filtered_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c3276e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for article in articles:\n",
    "    link = article.get_attribute(\"href\")\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da66fb9e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i, article in enumerate(articles):\n",
    "    link = article.get_attribute(\"href\")\n",
    "    print(f\"{i+1}. {link}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bb58e2",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "# Define keywords\n",
    "keywords = [\"fid\", \"rig\"]\n",
    "filtered_links = []\n",
    "\n",
    "# Filter relevant article links\n",
    "for article in articles:\n",
    "    link = article.get_attribute(\"href\")\n",
    "    if link and any(keyword in link.lower() for keyword in keywords):\n",
    "        filtered_links.append(link)\n",
    "\n",
    "# Print results\n",
    "print(f\"\\n Found {len(filtered_links)} relevant articles:\")\n",
    "for link in filtered_links:\n",
    "    print(link)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de19f523",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test !!!!!!!!!!!\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#  Start a fresh browser session\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for links to load (up to 10 seconds)\n",
    "try:\n",
    "    WebDriverWait(driver, 10).until(\n",
    "        EC.presence_of_all_elements_located((By.TAG_NAME, \"a\"))\n",
    "    )\n",
    "except Exception as e:\n",
    "    print(\" Timeout waiting for links to load:\", e)\n",
    "\n",
    "#  Find all article links\n",
    "articles = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "#  Define keywords\n",
    "keywords = [\"fid\", \"rig\"]\n",
    "filtered_links = set()  # Use set to avoid duplicates\n",
    "\n",
    "#  Filter relevant article links\n",
    "for article in articles:\n",
    "    try:\n",
    "        link = article.get_attribute(\"href\")\n",
    "        if link and any(keyword in link.lower() for keyword in keywords):\n",
    "            filtered_links.add(link)\n",
    "    except Exception as e:\n",
    "        print(f\" Error processing link: {e}\")\n",
    "        continue\n",
    "\n",
    "#  Print results\n",
    "print(f\"\\n Found {len(filtered_links)} relevant articles:\")\n",
    "for link in filtered_links:\n",
    "    print(link)\n",
    "\n",
    "#  Close the browser session\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8084012b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#  Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "#  Open the website\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "print(\" Browser launched successfully!\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be043c5c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#  Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"a\")))\n",
    "\n",
    "#  Find all article links\n",
    "articles = driver.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "#  Define keywords to filter articles\n",
    "keywords = [\"fid\", \"rig\"]\n",
    "filtered_links = set()  # Using a set to avoid duplicates\n",
    "\n",
    "#  Extract and filter links\n",
    "for article in articles:\n",
    "    link = article.get_attribute(\"href\")\n",
    "    if link and any(keyword in link.lower() for keyword in keywords):\n",
    "        filtered_links.add(link)\n",
    "\n",
    "#  Print results\n",
    "print(f\"\\n Found {len(filtered_links)} relevant articles:\")\n",
    "for link in filtered_links:\n",
    "    print(link)\n",
    "\n",
    "#  Close the browser session\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951a186d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "#  Set up Chrome WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for the page to load\n",
    "WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"a\")))\n",
    "\n",
    "#  Define keywords\n",
    "keywords = [\"fid\", \"rig\"]\n",
    "\n",
    "#  Find all article titles (Update the tag based on website inspection)\n",
    "articles = driver.find_elements(By.TAG_NAME, \"h2\")  # Change this if articles are in <h3>, <span>, etc.\n",
    "\n",
    "filtered_articles = []\n",
    "\n",
    "for article in articles:\n",
    "    try:\n",
    "        title = article.text.lower()  # Convert to lowercase for case-insensitive search\n",
    "        if any(keyword in title for keyword in keywords):\n",
    "            parent_link = article.find_element(By.XPATH, \"./ancestor::a\")  # Find the parent <a> tag\n",
    "            article_link = parent_link.get_attribute(\"href\")\n",
    "            filtered_articles.append((title, article_link))\n",
    "    except:\n",
    "        continue  # Skip if no link is found\n",
    "\n",
    "#  Print relevant articles\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, link in filtered_articles:\n",
    "    print(f\" {title}  {link}\")\n",
    "\n",
    "#  Close the browser session\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f33ed50",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#  Setup WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to load\n",
    "WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "#  Define keywords\n",
    "keywords = [\"fid\", \"rig\"]\n",
    "filtered_articles = []\n",
    "\n",
    "#  Extract article elements (Change the selector if needed)\n",
    "article_elements = driver.find_elements(By.XPATH, \"//h2 | //h3 | //span | //div\")  # Checks multiple tags\n",
    "\n",
    "for article in article_elements:\n",
    "    title = article.text.lower().strip()\n",
    "    if any(keyword in title for keyword in keywords):\n",
    "        try:\n",
    "            parent_link = article.find_element(By.XPATH, \"./ancestor::a\")  # Get parent <a> tag\n",
    "            article_link = parent_link.get_attribute(\"href\")\n",
    "        except:\n",
    "            article_link = \"No link found\"  # Sometimes articles don't have links\n",
    "\n",
    "        filtered_articles.append((title, article_link))\n",
    "\n",
    "#  Print results\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, link in filtered_articles:\n",
    "    print(f\" {title}  {link}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dae56dc",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#  Setup WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to load\n",
    "WebDriverWait(driver, 15).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "#  Extract all visible text\n",
    "all_text = driver.find_element(By.TAG_NAME, \"body\").text\n",
    "\n",
    "#  Print the first 1000 characters to debug\n",
    "print(\"\\n DEBUG - Extracted Text from Website:\\n\")\n",
    "print(all_text[:1000])  # Printing only first 1000 characters to avoid too much output\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb44166",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#  Setup WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "#  Extract article headings (try different elements if needed)\n",
    "headings = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //p\")  # Modify based on website structure\n",
    "\n",
    "#  Debugging output - Print first 10 headlines\n",
    "print(\"\\n DEBUG - Extracted Headlines:\\n\")\n",
    "for i, heading in enumerate(headings[:10]):  # Limit to first 10 for readability\n",
    "    print(f\"{i+1}. {heading.text}\")\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a039dee8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "time.sleep(5)  # Wait for page to load fully\n",
    "\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "#  STEP 2: Extract Headlines\n",
    "headings = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //p\")\n",
    "\n",
    "print(\"\\n DEBUG - Extracted Headlines:\\n\")\n",
    "for i, heading in enumerate(headings[:10]):  # Display first 10 extracted headlines\n",
    "    print(f\"{i+1}. {heading.text}\")\n",
    "\n",
    "#  STEP 3: Filter for Relevant Articles\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "filtered_articles = [h.text for h in headings if any(k in h.text.lower() for k in keywords)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for article in filtered_articles:\n",
    "    print(article)\n",
    "\n",
    "#  STEP 4: If No Relevant Articles, Scroll and Extract Again\n",
    "if len(filtered_articles) == 0:\n",
    "    print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "    for _ in range(5):  # Scroll down 5 times\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Wait for new content to load\n",
    "\n",
    "    #  Extract Headlines Again After Scrolling\n",
    "    headings = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //p\")\n",
    "    \n",
    "    print(\"\\n DEBUG AFTER SCROLLING - Extracted Headlines:\\n\")\n",
    "    for i, heading in enumerate(headings[:10]):  # Display first 10 after scrolling\n",
    "        print(f\"{i+1}. {heading.text}\")\n",
    "\n",
    "    #  Re-run filtering\n",
    "    filtered_articles = [h.text for h in headings if any(k in h.text.lower() for k in keywords)]\n",
    "    \n",
    "    print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles:\")\n",
    "    for article in filtered_articles:\n",
    "        print(article)\n",
    "\n",
    "#  Close the browser session\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2ba200",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs (with better error handling)\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links from the page with error handling.\"\"\"\n",
    "    extracted = []\n",
    "    \n",
    "    # Extract elements containing article headlines\n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            # Try to find the closest <a> tag\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  # No <a> parent found, continue\n",
    "\n",
    "        # If URL is missing, check if it's an <a> tag itself\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        if title and url:\n",
    "            extracted.append((title, url))\n",
    "\n",
    "    return extracted\n",
    "\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "#  Debug: Show Extracted Headlines\n",
    "print(\"\\n DEBUG - Extracted Headlines & URLs:\\n\")\n",
    "for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "    print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  STEP 4: If No Relevant Articles, Scroll & Try Again\n",
    "if not filtered_articles:\n",
    "    print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Allow time for loading\n",
    "\n",
    "        new_articles = extract_articles()\n",
    "\n",
    "        #  Debugging: Print number of newly extracted articles\n",
    "        print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "        # Stop scrolling if no new content is loading\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    #  Re-run filtering after scrolling\n",
    "    articles = extract_articles()\n",
    "\n",
    "    print(\"\\nDEBUG AFTER SCROLLING - Extracted Headlines & URLs:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "    \n",
    "    print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086e648",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs (Better Filtering)\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with improved filtering.\"\"\"\n",
    "    extracted = set()  # Use a set to store unique articles\n",
    "    \n",
    "    # Extract elements containing article headlines\n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            # Find nearest parent <a> tag to extract the URL\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  # If no parent <a> tag, skip\n",
    "\n",
    "        # If URL is missing but the headline itself is a link\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        # Skip irrelevant links (like menu items)\n",
    "        if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "            extracted.add((title, url))  # Store only unique articles\n",
    "\n",
    "    return list(extracted)\n",
    "\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "#  Debug: Show Extracted Headlines\n",
    "print(\"\\n Extracted Headlines & URLs:\\n\")\n",
    "for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "    print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  STEP 4: If No Relevant Articles, Scroll & Try Again\n",
    "if not filtered_articles:\n",
    "    print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Allow time for loading\n",
    "\n",
    "        new_articles = extract_articles()\n",
    "\n",
    "        #  Debugging: Print number of newly extracted articles\n",
    "        print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "        # Stop scrolling if no new content is loading\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    #  Re-run filtering after scrolling\n",
    "    articles = extract_articles()\n",
    "\n",
    "    print(\"\\n DEBUG AFTER SCROLLING - Extracted Headlines & URLs:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "    \n",
    "    print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac63240f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install selenium webdriver-manager beautifulsoup4 requests torch transformers\n",
    "pip install torch torchvision torchaudio\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62671b2a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "import requests\n",
    "import time\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs (Better Filtering)\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with improved filtering.\"\"\"\n",
    "    extracted = set()  # Use a set to store unique articles\n",
    "    \n",
    "    # Extract elements containing article headlines\n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            # Find nearest parent <a> tag to extract the URL\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  # If no parent <a> tag, skip\n",
    "\n",
    "        # If URL is missing but the headline itself is a link\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        # Skip irrelevant links (like menu items)\n",
    "        if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "            extracted.add((title, url))  # Store only unique articles\n",
    "\n",
    "    return list(extracted)\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "#  Debug: Show Extracted Headlines\n",
    "print(\"\\n Extracted Headlines & URLs:\\n\")\n",
    "for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "    print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  STEP 4: If No Relevant Articles, Scroll & Try Again\n",
    "if not filtered_articles:\n",
    "    print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Allow time for loading\n",
    "\n",
    "        new_articles = extract_articles()\n",
    "\n",
    "        #  Debugging: Print number of newly extracted articles\n",
    "        print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "        # Stop scrolling if no new content is loading\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    #  Re-run filtering after scrolling\n",
    "    articles = extract_articles()\n",
    "\n",
    "    print(\"\\n DEBUG AFTER SCROLLING - Extracted Headlines & URLs:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "    \n",
    "    print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close browser after extracting URLs\n",
    "driver.quit()\n",
    "\n",
    "#  STEP 5: Fetch Article Content & Summarize\n",
    "def fetch_article_text(url):\n",
    "    \"\"\"Fetches and extracts main content from an article URL.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            print(f\" Skipping {url} (Status Code: {response.status_code})\")\n",
    "            return \"\"\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Try different HTML tags that might contain the main text\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        content = \" \".join(p.get_text() for p in paragraphs if p.get_text())\n",
    "\n",
    "        return content if content else \" No extractable content found.\"\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "#  Load Hugging Face Summarization Model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "#  Generate Summaries\n",
    "print(\"\\n Summarizing Relevant Articles:\\n\")\n",
    "for title, url in filtered_articles:\n",
    "    article_text = fetch_article_text(url)\n",
    "    \n",
    "    if article_text and article_text != \" No extractable content found.\":\n",
    "        summary = summarizer(article_text, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "        print(f\" **{title}**\\n {url}\\n Summary: {summary}\\n\")\n",
    "    else:\n",
    "        print(f\" **{title}**\\n {url}\\n Summary:  No summary available.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c8c97e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d6fc6a",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from transformers import pipeline\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs (Better Filtering)\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with improved filtering.\"\"\"\n",
    "    extracted = set()  # Use a set to store unique articles\n",
    "    \n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  \n",
    "\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "            extracted.add((title, url))  \n",
    "\n",
    "    return list(extracted)\n",
    "\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "print(\"\\nExtracted Headlines & URLs:\\n\")\n",
    "for i, (title, url) in enumerate(articles[:10]):  \n",
    "    print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  STEP 4: If No Relevant Articles, Scroll & Try Again\n",
    "if not filtered_articles:\n",
    "    print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  \n",
    "\n",
    "        new_articles = extract_articles()\n",
    "\n",
    "        print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    articles = extract_articles()\n",
    "\n",
    "    print(\"\\n DEBUG AFTER SCROLLING - Extracted Headlines & URLs:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  \n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "    \n",
    "    print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close browser\n",
    "driver.quit()\n",
    "\n",
    "\n",
    "#  STEP 5: Fetch & Summarize Articles\n",
    "def fetch_article_text(url):\n",
    "    \"\"\"Fetches and extracts article text using BeautifulSoup.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        article_text = \" \".join(p.get_text() for p in paragraphs)\n",
    "\n",
    "        return article_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "#  Load Summarization Model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "#  Generate Summaries\n",
    "print(\"\\n Summarizing Relevant Articles:\\n\")\n",
    "\n",
    "for title, url in filtered_articles:\n",
    "    article_text = fetch_article_text(url)\n",
    "\n",
    "    if len(article_text) < 200:\n",
    "        print(f\" Not enough text found for {title}, skipping...\\n\")\n",
    "        continue\n",
    "\n",
    "    summary = summarizer(article_text, max_length=150, min_length=50, do_sample=False)\n",
    "    summarized_text = summary[0]['summary_text']\n",
    "\n",
    "    print(f\" {title}\")\n",
    "    print(f\" {url}\")\n",
    "    print(f\" Summary: {summarized_text}\\n\")\n",
    "    print(\"=\"*80)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e2d89",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from transformers import pipeline\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with improved filtering.\"\"\"\n",
    "    extracted = set()  # Use a set to store unique articles\n",
    "    \n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  \n",
    "\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "            extracted.add((title, url)) \n",
    "\n",
    "    return list(extracted)\n",
    "\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close WebDriver (No need for further Selenium usage)\n",
    "driver.quit()\n",
    "\n",
    "#  STEP 4: Extract Full Article Text\n",
    "def fetch_article_text(url):\n",
    "    \"\"\"Fetches and extracts full text from an article URL using BeautifulSoup.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Try common article text containers\n",
    "        paragraphs = soup.find_all(\"p\")\n",
    "        article_text = \" \".join(p.text for p in paragraphs if len(p.text) > 30)  # Ignore short text\n",
    "\n",
    "        return article_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "#  Load Summarization Model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "#  Generate Summaries\n",
    "print(\"\\n Summarizing Relevant Articles:\\n\")\n",
    "\n",
    "for title, url in filtered_articles:\n",
    "    full_text = fetch_article_text(url)\n",
    "\n",
    "    if not full_text:\n",
    "        print(f\" No extractable text found for: {title} ({url})\")\n",
    "        continue\n",
    "\n",
    "    # Limit input to avoid token overflow (BART can handle ~1024 tokens)\n",
    "    full_text = full_text[:2000]\n",
    "\n",
    "    summary = summarizer(full_text, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    print(f\" {title}\\n {url}\\n Summary: {summary}\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64afeda4",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "from transformers import pipeline\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with improved filtering.\"\"\"\n",
    "    extracted = set()  # Use a set to store unique articles\n",
    "    \n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  \n",
    "\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "            extracted.add((title, url)) \n",
    "\n",
    "    return list(extracted)\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close WebDriver (No need for further Selenium usage)\n",
    "driver.quit()\n",
    "\n",
    "#  STEP 4: Extract Full Article Text\n",
    "def fetch_article_text(url):\n",
    "    \"\"\"Fetches and extracts full text from an article URL using BeautifulSoup.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        # Try to locate the correct article body section\n",
    "        content_containers = soup.find_all(\"p\")  # Modify this selector as per site structure\n",
    "\n",
    "        article_text = \" \".join(p.text for p in content_containers if len(p.text) > 30)  # Ignore short text\n",
    "\n",
    "        if not article_text:\n",
    "            return \"\"\n",
    "\n",
    "        return article_text.strip()\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "#  Load Summarization Model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "#  Generate Summaries\n",
    "print(\"\\n Summarizing Relevant Articles:\\n\")\n",
    "\n",
    "for title, url in filtered_articles:\n",
    "    full_text = fetch_article_text(url)\n",
    "\n",
    "    if not full_text:\n",
    "        print(f\" No extractable text found for: {title} ({url})\")\n",
    "        continue\n",
    "\n",
    "    # Limit input to avoid token overflow (BART can handle ~1024 tokens)\n",
    "    full_text = full_text[:2000]\n",
    "\n",
    "    summary = summarizer(full_text, max_length=150, min_length=50, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    print(f\" {title}\\n {url}\\n Summary: {summary}\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69103ead",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from transformers import pipeline\n",
    "\n",
    "#  FIX: Disable Hugging Face tokenizer parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with improved filtering.\"\"\"\n",
    "    extracted = set()\n",
    "    \n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  \n",
    "\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "            extracted.add((title, url)) \n",
    "\n",
    "    return list(extracted)\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "#  STEP 4: Extract Full Article Text (FIXED)\n",
    "def fetch_article_text(url):\n",
    "    \"\"\"Fetches and extracts full text from an article URL using BeautifulSoup.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, headers={\"User-Agent\": \"Mozilla/5.0\"})\n",
    "        if response.status_code != 200:\n",
    "            return \"\"\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        #  FIX: Use more precise selectors for article body\n",
    "        possible_containers = [\n",
    "            soup.find(\"article\"),  # Try to locate <article> tag\n",
    "            soup.find(\"div\", class_=\"article-body\"),  # Adjust based on site's class names\n",
    "            soup.find(\"section\", class_=\"content\")  # Adjust if necessary\n",
    "        ]\n",
    "\n",
    "        article_text = \"\"\n",
    "        for container in possible_containers:\n",
    "            if container:\n",
    "                article_text = \" \".join(p.text for p in container.find_all(\"p\") if len(p.text) > 30)\n",
    "                break  \n",
    "\n",
    "        return article_text.strip() if article_text else \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error fetching {url}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "#  Load Summarization Model\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "#  Generate Summaries\n",
    "print(\"\\n Summarizing Relevant Articles:\\n\")\n",
    "\n",
    "for title, url in filtered_articles:\n",
    "    full_text = fetch_article_text(url)\n",
    "\n",
    "    if not full_text:\n",
    "        print(f\" No extractable text found for: {title} ({url})\")\n",
    "        continue\n",
    "\n",
    "    #  FIX: Adjust summarization length dynamically\n",
    "    max_summary_length = min(150, len(full_text) // 2)  # Max length is half the article text\n",
    "    min_summary_length = max(50, len(full_text) // 4)  # Min length is quarter of the article text\n",
    "\n",
    "    summary = summarizer(full_text, max_length=max_summary_length, min_length=min_summary_length, do_sample=False)[0]['summary_text']\n",
    "\n",
    "    print(f\" {title}\\n {url}\\n Summary: {summary}\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb323e4e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Keywords for Filtering\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "\n",
    "#  Launch WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "all_filtered_articles = []  # To store results from all websites\n",
    "\n",
    "for website in websites:\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "    driver.get(website)\n",
    "\n",
    "    #  Wait for page to fully load\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "    except:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "        continue\n",
    "\n",
    "    #  Extract Articles Function\n",
    "    def extract_articles():\n",
    "        \"\"\"Extracts article headlines & links from the page with error handling.\"\"\"\n",
    "        extracted = []\n",
    "        \n",
    "        headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "        for headline in headlines:\n",
    "            title = headline.text.strip()\n",
    "            url = None\n",
    "\n",
    "            try:\n",
    "                parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "                url = parent.get_attribute(\"href\")\n",
    "            except:\n",
    "                pass  # No <a> parent found, continue\n",
    "\n",
    "            if not url and headline.tag_name == \"a\":\n",
    "                url = headline.get_attribute(\"href\")\n",
    "\n",
    "            if title and url:\n",
    "                extracted.append((title, url))\n",
    "\n",
    "        return extracted\n",
    "\n",
    "    #  Extract Articles from the Page\n",
    "    articles = extract_articles()\n",
    "\n",
    "    #  Debugging: Show Extracted Headlines\n",
    "    print(f\"\\n DEBUG - Extracted Headlines from {website}:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    #  Filter Relevant Articles\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "    print(f\"\\n Found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "    #  Scroll & Retry If No Relevant Articles Found\n",
    "    if not filtered_articles:\n",
    "        print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(2)  # Allow time for loading\n",
    "\n",
    "            new_articles = extract_articles()\n",
    "            print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        articles = extract_articles()\n",
    "\n",
    "        print(f\"\\n DEBUG AFTER SCROLLING - Extracted Headlines from {website}:\\n\")\n",
    "        for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "            print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "        \n",
    "        print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "        for title, url in filtered_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "    #  Save Results\n",
    "    all_filtered_articles.extend(filtered_articles)\n",
    "\n",
    "#  Close browser after scraping all websites\n",
    "driver.quit()\n",
    "\n",
    "#  Final Output\n",
    "print(\"\\n SUMMARY - Total Relevant Articles Found Across All Websites:\")\n",
    "for title, url in all_filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c3662cd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Keywords for Filtering\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "\n",
    "#  Launch WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "all_filtered_articles = []  # To store results from all websites\n",
    "\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links from the page with error handling.\"\"\"\n",
    "    extracted = []\n",
    "    \n",
    "    try:\n",
    "        #  Wait for headlines to load properly\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\"))\n",
    "        )\n",
    "        \n",
    "        headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "        for headline in headlines:\n",
    "            try:\n",
    "                title = headline.text.strip()\n",
    "                url = None\n",
    "\n",
    "                # Check for parent <a> element\n",
    "                try:\n",
    "                    parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "                    url = parent.get_attribute(\"href\")\n",
    "                except:\n",
    "                    pass  # No parent <a> found\n",
    "\n",
    "                # If no parent <a>, check if the headline itself is a link\n",
    "                if not url and headline.tag_name == \"a\":\n",
    "                    url = headline.get_attribute(\"href\")\n",
    "\n",
    "                if title and url:\n",
    "                    extracted.append((title, url))\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                print(\" StaleElementReferenceException - Retrying headline extraction...\")\n",
    "                continue  # Skip stale elements and continue\n",
    "\n",
    "    except (StaleElementReferenceException, TimeoutException):\n",
    "        print(\" StaleElementReferenceException - Retrying full article extraction...\")\n",
    "        return extract_articles()  # Retry extraction\n",
    "\n",
    "    return extracted\n",
    "\n",
    "for website in websites:\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "    driver.get(website)\n",
    "\n",
    "    #  Wait for page to fully load\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "        continue\n",
    "\n",
    "    #  Extract Articles from the Page\n",
    "    articles = extract_articles()\n",
    "\n",
    "    #  Debugging: Show Extracted Headlines\n",
    "    print(f\"\\n DEBUG - Extracted Headlines from {website}:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    #  Filter Relevant Articles\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "    print(f\"\\n Found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "    #  Scroll & Retry If No Relevant Articles Found\n",
    "    if not filtered_articles:\n",
    "        print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(2)  # Allow time for loading\n",
    "\n",
    "            new_articles = extract_articles()\n",
    "            print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        articles = extract_articles()\n",
    "\n",
    "        print(f\"\\n DEBUG AFTER SCROLLING - Extracted Headlines from {website}:\\n\")\n",
    "        for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "            print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "        \n",
    "        print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "        for title, url in filtered_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "    #  Save Results\n",
    "    all_filtered_articles.extend(filtered_articles)\n",
    "\n",
    "#  Close browser after scraping all websites\n",
    "driver.quit()\n",
    "\n",
    "#  Final Output\n",
    "print(\"\\n SUMMARY - Total Relevant Articles Found Across All Websites:\")\n",
    "for title, url in all_filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdccca8",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Keywords for Filtering\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "\n",
    "#  Enable Safari WebDriver (Run this command in Terminal first)\n",
    "# safaridriver --enable\n",
    "\n",
    "#  Launch Safari WebDriver\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "all_filtered_articles = []  # Store results from all websites\n",
    "\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links from the page with error handling.\"\"\"\n",
    "    extracted = []\n",
    "    \n",
    "    try:\n",
    "        #  Wait for headlines to load properly\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\"))\n",
    "        )\n",
    "        \n",
    "        headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "        for headline in headlines:\n",
    "            try:\n",
    "                title = headline.text.strip()\n",
    "                url = None\n",
    "\n",
    "                # Check for parent <a> element\n",
    "                try:\n",
    "                    parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "                    url = parent.get_attribute(\"href\")\n",
    "                except:\n",
    "                    pass  # No parent <a> found\n",
    "\n",
    "                # If no parent <a>, check if the headline itself is a link\n",
    "                if not url and headline.tag_name == \"a\":\n",
    "                    url = headline.get_attribute(\"href\")\n",
    "\n",
    "                if title and url:\n",
    "                    extracted.append((title, url))\n",
    "\n",
    "            except StaleElementReferenceException:\n",
    "                print(\" StaleElementReferenceException - Retrying headline extraction...\")\n",
    "                continue  # Skip stale elements and continue\n",
    "\n",
    "    except (StaleElementReferenceException, TimeoutException):\n",
    "        print(\" StaleElementReferenceException - Retrying full article extraction...\")\n",
    "        return extract_articles()  # Retry extraction\n",
    "\n",
    "    return extracted\n",
    "\n",
    "for website in websites:\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "    driver.get(website)\n",
    "\n",
    "    #  Wait for page to fully load\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "        continue\n",
    "\n",
    "    #  Extract Articles from the Page\n",
    "    articles = extract_articles()\n",
    "\n",
    "    #  Debugging: Show Extracted Headlines\n",
    "    print(f\"\\n DEBUG - Extracted Headlines from {website}:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    #  Filter Relevant Articles\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "    print(f\"\\n Found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "    #  Scroll & Retry If No Relevant Articles Found\n",
    "    if not filtered_articles:\n",
    "        print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(2)  # Allow time for loading\n",
    "\n",
    "            new_articles = extract_articles()\n",
    "            print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        articles = extract_articles()\n",
    "\n",
    "        print(f\"\\n DEBUG AFTER SCROLLING - Extracted Headlines from {website}:\\n\")\n",
    "        for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "            print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "        \n",
    "        print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "        for title, url in filtered_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "    #  Save Results\n",
    "    all_filtered_articles.extend(filtered_articles)\n",
    "\n",
    "#  Close browser after scraping all websites\n",
    "driver.quit()\n",
    "\n",
    "#  Final Output\n",
    "print(\"\\n SUMMARY - Total Relevant Articles Found Across All Websites:\")\n",
    "for title, url in all_filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3650e9d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Keywords for Filtering\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "\n",
    "#  Enable Safari WebDriver (Run this command in Terminal first)\n",
    "# safaridriver --enable\n",
    "\n",
    "#  Launch Safari WebDriver\n",
    "driver = webdriver.Safari()\n",
    "\n",
    "all_filtered_articles = []  # Store results from all websites\n",
    "\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links from the page with better handling.\"\"\"\n",
    "    extracted = []\n",
    "    \n",
    "    try:\n",
    "        #  Wait for headlines to load properly\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.TAG_NAME, \"body\"))\n",
    "        )\n",
    "\n",
    "        #  Use JavaScript to get all potential article elements\n",
    "        headlines = driver.execute_script(\"\"\"\n",
    "            return Array.from(document.querySelectorAll('h1, h2, h3, h4, span, div'))\n",
    "                .map(el => ({ text: el.innerText.trim(), href: el.closest('a') ? el.closest('a').href : null }))\n",
    "                .filter(el => el.text && el.href);\n",
    "        \"\"\")\n",
    "\n",
    "        for item in headlines:\n",
    "            title, url = item[\"text\"], item[\"href\"]\n",
    "            if title and url:\n",
    "                extracted.append((title, url))\n",
    "\n",
    "    except (StaleElementReferenceException, TimeoutException):\n",
    "        print(\" Retrying full article extraction due to stale elements...\")\n",
    "        return extract_articles()  # Retry extraction\n",
    "    \n",
    "    return extracted\n",
    "\n",
    "for website in websites:\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "    driver.get(website)\n",
    "\n",
    "    #  Wait for page to fully load\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "        continue\n",
    "\n",
    "    #  Extract Articles from the Page\n",
    "    articles = extract_articles()\n",
    "\n",
    "    #  Debugging: Show Extracted Headlines\n",
    "    print(f\"\\n DEBUG - Extracted Headlines from {website}:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    #  Filter Relevant Articles\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "    print(f\"\\n Found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "    #  Scroll & Retry If No Relevant Articles Found\n",
    "    if not filtered_articles:\n",
    "        print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "        last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "            time.sleep(2)  # Allow time for loading\n",
    "\n",
    "            new_articles = extract_articles()\n",
    "            print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "            new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "            if new_height == last_height:\n",
    "                break\n",
    "            last_height = new_height\n",
    "\n",
    "        articles = extract_articles()\n",
    "\n",
    "        print(f\"\\n DEBUG AFTER SCROLLING - Extracted Headlines from {website}:\\n\")\n",
    "        for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "            print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "        \n",
    "        print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "        for title, url in filtered_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "    #  Save Results\n",
    "    all_filtered_articles.extend(filtered_articles)\n",
    "\n",
    "#  Close browser after scraping all websites\n",
    "driver.quit()\n",
    "\n",
    "#  Final Output\n",
    "print(\"\\n SUMMARY - Total Relevant Articles Found Across All Websites:\")\n",
    "for title, url in all_filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681c6713",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Keywords for Filtering (case-insensitive, regex support)\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "keyword_pattern = re.compile(r'\\b(?:' + '|'.join(keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "#  Set to store unique articles\n",
    "unique_articles = set()\n",
    "\n",
    "#  Function to initialize Selenium WebDriver\n",
    "def init_driver():\n",
    "    driver = webdriver.Safari()\n",
    "    driver.set_page_load_timeout(20)  # Set timeout to avoid hanging\n",
    "    return driver\n",
    "\n",
    "#  Function to extract article titles and links\n",
    "def extract_articles(driver):\n",
    "    \"\"\"Extracts article headlines and links from the page with robust error handling.\"\"\"\n",
    "    extracted = []\n",
    "    retries = 3  # Retry extraction in case of stale elements\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            #  Wait for page content to load\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            \n",
    "            #  Extract article elements using JavaScript\n",
    "            headlines = driver.execute_script(\"\"\"\n",
    "                return Array.from(document.querySelectorAll('h1, h2, h3, h4, span, div'))\n",
    "                    .map(el => ({ text: el.innerText.trim(), href: el.closest('a') ? el.closest('a').href : null }))\n",
    "                    .filter(el => el.text && el.href);\n",
    "            \"\"\")\n",
    "\n",
    "            for item in headlines:\n",
    "                title, url = item[\"text\"], item[\"href\"]\n",
    "                if title and url:\n",
    "                    extracted.append((title, url))\n",
    "\n",
    "            break  # If successful, exit retry loop\n",
    "\n",
    "        except (StaleElementReferenceException, TimeoutException):\n",
    "            print(f\" Retrying extraction... ({attempt+1}/{retries})\")\n",
    "            time.sleep(2)  # Short delay before retrying\n",
    "\n",
    "    return extracted\n",
    "\n",
    "#  Function to scroll and load more articles dynamically\n",
    "def scroll_and_extract(driver):\n",
    "    \"\"\"Scrolls the page to load more articles and extracts them.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Allow time for content to load\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # Stop if no new content loads\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    return extract_articles(driver)\n",
    "\n",
    "#  Function to scrape a single website\n",
    "def scrape_website(website):\n",
    "    \"\"\"Scrapes a single website for articles matching keywords.\"\"\"\n",
    "    driver = init_driver()\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "\n",
    "    try:\n",
    "        driver.get(website)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "\n",
    "        #  Extract Articles from the Page\n",
    "        articles = extract_articles(driver)\n",
    "\n",
    "        #  If no relevant articles found, scroll to load more\n",
    "        if not any(keyword_pattern.search(title) for title, _ in articles):\n",
    "            print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "            articles = scroll_and_extract(driver)\n",
    "\n",
    "        #  Filter relevant articles using regex\n",
    "        filtered_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "\n",
    "        print(f\"\\n Found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "        for title, url in filtered_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "        #  Save unique results\n",
    "        for title, url in filtered_articles:\n",
    "            unique_articles.add((title, url))\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "#  Run scraping in parallel using ThreadPoolExecutor\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_website, websites)\n",
    "\n",
    "    #  Final Output\n",
    "    print(\"\\n SUMMARY - Total Relevant Articles Found Across All Websites:\")\n",
    "    for title, url in unique_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b629cab",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Updated Keywords List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Compile regex for efficient keyword matching\n",
    "keyword_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(k) for k in keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "#  Set to store unique articles\n",
    "unique_articles = set()\n",
    "\n",
    "#  Function to initialize Selenium WebDriver\n",
    "def init_driver():\n",
    "    driver = webdriver.Safari()\n",
    "    driver.set_page_load_timeout(20)  # Set timeout to avoid hanging\n",
    "    return driver\n",
    "\n",
    "#  Function to extract article titles and links\n",
    "def extract_articles(driver):\n",
    "    \"\"\"Extracts article headlines and links from the page with robust error handling.\"\"\"\n",
    "    extracted = []\n",
    "    retries = 3  # Retry extraction in case of stale elements\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            #  Wait for page content to load\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "            \n",
    "            #  Extract article elements using JavaScript\n",
    "            headlines = driver.execute_script(\"\"\"\n",
    "                return Array.from(document.querySelectorAll('h1, h2, h3, h4, span, div'))\n",
    "                    .map(el => ({ text: el.innerText.trim(), href: el.closest('a') ? el.closest('a').href : null }))\n",
    "                    .filter(el => el.text && el.href);\n",
    "            \"\"\")\n",
    "\n",
    "            for item in headlines:\n",
    "                title, url = item[\"text\"], item[\"href\"]\n",
    "                if title and url:\n",
    "                    extracted.append((title, url))\n",
    "\n",
    "            break  # If successful, exit retry loop\n",
    "\n",
    "        except (StaleElementReferenceException, TimeoutException):\n",
    "            print(f\" Retrying extraction... ({attempt+1}/{retries})\")\n",
    "            time.sleep(2)  # Short delay before retrying\n",
    "\n",
    "    return extracted\n",
    "\n",
    "#  Function to scroll and load more articles dynamically\n",
    "def scroll_and_extract(driver):\n",
    "    \"\"\"Scrolls the page to load more articles and extracts them.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Allow time for content to load\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break  # Stop if no new content loads\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    return extract_articles(driver)\n",
    "\n",
    "#  Function to scrape a single website\n",
    "def scrape_website(website):\n",
    "    \"\"\"Scrapes a single website for articles matching keywords.\"\"\"\n",
    "    driver = init_driver()\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "\n",
    "    try:\n",
    "        driver.get(website)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "\n",
    "        #  Extract Articles from the Page\n",
    "        articles = extract_articles(driver)\n",
    "\n",
    "        #  If no relevant articles found, scroll to load more\n",
    "        if not any(keyword_pattern.search(title) for title, _ in articles):\n",
    "            print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "            articles = scroll_and_extract(driver)\n",
    "\n",
    "        #  Filter relevant articles using regex\n",
    "        filtered_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "\n",
    "        print(f\"\\n Found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "        for title, url in filtered_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "        #  Save unique results\n",
    "        for title, url in filtered_articles:\n",
    "            unique_articles.add((title, url))\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "#  Run scraping in parallel using ThreadPoolExecutor\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_website, websites)\n",
    "\n",
    "    #  Final Output: URLs of extracted articles\n",
    "    print(\"\\n SUMMARY - Total Relevant Articles Found Across All Websites:\")\n",
    "    for title, url in unique_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd6a835",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs (Better Filtering)\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with improved filtering.\"\"\"\n",
    "    extracted = set()  # Use a set to store unique articles\n",
    "    \n",
    "    # Extract elements containing article headlines\n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            # Find nearest parent <a> tag to extract the URL\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  # If no parent <a> tag, skip\n",
    "\n",
    "        # If URL is missing but the headline itself is a link\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        # Skip irrelevant links (like menu items)\n",
    "        if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "            extracted.add((title, url))  # Store only unique articles\n",
    "\n",
    "    return list(extracted)\n",
    "\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "#  Debug: Show Extracted Headlines\n",
    "print(\"\\n Extracted Headlines & URLs:\\n\")\n",
    "for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "    print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "keywords = [\"rig\", \"fid\"]\n",
    "filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  STEP 4: If No Relevant Articles, Scroll & Try Again\n",
    "if not filtered_articles:\n",
    "    print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Allow time for loading\n",
    "\n",
    "        new_articles = extract_articles()\n",
    "\n",
    "        #  Debugging: Print number of newly extracted articles\n",
    "        print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "        # Stop scrolling if no new content is loading\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    #  Re-run filtering after scrolling\n",
    "    articles = extract_articles()\n",
    "\n",
    "    print(\"\\n DEBUG AFTER SCROLLING - Extracted Headlines & URLs:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k in title.lower() for k in keywords)]\n",
    "    \n",
    "    print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f53caa",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Regex for Multi-Word Matching\n",
    "keyword_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(k) for k in keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "#  Set to store unique articles\n",
    "unique_articles = set()\n",
    "\n",
    "#  Function to initialize Selenium WebDriver\n",
    "def init_driver():\n",
    "    driver = webdriver.Safari()\n",
    "    driver.set_page_load_timeout(20)  # Avoid hanging\n",
    "    return driver\n",
    "\n",
    "#  Function to extract article titles and links\n",
    "def extract_articles(driver):\n",
    "    \"\"\"Extracts article headlines and links with improved coverage.\"\"\"\n",
    "    extracted = []\n",
    "    retries = 3  # Retry extraction in case of stale elements\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            #  Expanded JavaScript Query for More Complete Extraction\n",
    "            headlines = driver.execute_script(\"\"\"\n",
    "                return Array.from(document.querySelectorAll('h1, h2, h3, h4, span, div, p, a'))\n",
    "                    .map(el => ({ text: el.innerText.trim(), href: el.closest('a') ? el.closest('a').href : null }))\n",
    "                    .filter(el => el.text && el.href);\n",
    "            \"\"\")\n",
    "\n",
    "            for item in headlines:\n",
    "                title, url = item[\"text\"], item[\"href\"]\n",
    "                if title and url:\n",
    "                    extracted.append((title, url))\n",
    "\n",
    "            break  # If successful, exit retry loop\n",
    "\n",
    "        except (StaleElementReferenceException, TimeoutException):\n",
    "            print(f\" Retrying extraction... ({attempt+1}/{retries})\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    return extracted\n",
    "\n",
    "#  Function to scroll and extract more articles\n",
    "def scroll_and_extract(driver):\n",
    "    \"\"\"Scrolls the page multiple times to load more articles.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(3)  # Allow loading time\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    return extract_articles(driver)\n",
    "\n",
    "#  Function to scrape a website\n",
    "def scrape_website(website):\n",
    "    \"\"\"Scrapes articles from a website using Selenium.\"\"\"\n",
    "    driver = init_driver()\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "\n",
    "    try:\n",
    "        driver.get(website)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "\n",
    "        #  Extract Articles from the Page\n",
    "        articles = extract_articles(driver)\n",
    "        print(f\" Extracted {len(articles)} articles before filtering.\")\n",
    "\n",
    "        #  Scroll if needed\n",
    "        if not any(keyword_pattern.search(title) for title, _ in articles):\n",
    "            print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "            articles = scroll_and_extract(driver)\n",
    "\n",
    "        #  Debug: Print All Extracted Articles Before Filtering\n",
    "        print(\"\\n DEBUG: Extracted Articles (Before Filtering):\")\n",
    "        for title, url in articles[:10]:  # Show first 10 for debugging\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "        #  Filter relevant articles using regex\n",
    "        filtered_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "\n",
    "        print(f\"\\n Found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "        for title, url in filtered_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "        #  Save unique results\n",
    "        for title, url in filtered_articles:\n",
    "            unique_articles.add((title, url))\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "#  Run scraping in parallel using ThreadPoolExecutor\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_website, websites)\n",
    "\n",
    "    #  Final Output: Only Relevant Articles\n",
    "    print(\"\\n SUMMARY - All Relevant Articles Found:\")\n",
    "    for title, url in unique_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5932950",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Regex for Multi-Word Matching\n",
    "keyword_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(k) for k in keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "#  Set to store unique articles\n",
    "unique_articles = set()\n",
    "\n",
    "#  Function to initialize Selenium WebDriver\n",
    "def init_driver():\n",
    "    driver = webdriver.Safari()\n",
    "    driver.set_page_load_timeout(30)  # Avoid hanging\n",
    "    return driver\n",
    "\n",
    "#  Function to extract article titles and links\n",
    "def extract_articles(driver):\n",
    "    \"\"\"Extracts article headlines and links with improved coverage.\"\"\"\n",
    "    extracted = []\n",
    "    retries = 3  # Retry extraction in case of stale elements\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            #  Expanded JavaScript Query for More Complete Extraction\n",
    "            headlines = driver.execute_script(\"\"\"\n",
    "                return Array.from(document.querySelectorAll('h1, h2, h3, h4, span, div, p, a'))\n",
    "                    .map(el => ({ text: el.innerText.trim(), href: el.closest('a') ? el.closest('a').href : null }))\n",
    "                    .filter(el => el.text && el.href);\n",
    "            \"\"\")\n",
    "\n",
    "            for item in headlines:\n",
    "                title, url = item[\"text\"], item[\"href\"]\n",
    "                if title and url:\n",
    "                    extracted.append((title, url))\n",
    "\n",
    "            break  # If successful, exit retry loop\n",
    "\n",
    "        except (StaleElementReferenceException, TimeoutException):\n",
    "            print(f\" Retrying extraction... ({attempt+1}/{retries})\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    return extracted\n",
    "\n",
    "#  Function to scroll and extract more articles\n",
    "def scroll_and_extract(driver):\n",
    "    \"\"\"Scrolls the page multiple times to load more articles.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(3)  # Allow loading time\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    return extract_articles(driver)\n",
    "\n",
    "#  Function to scrape a website\n",
    "def scrape_website(website):\n",
    "    \"\"\"Scrapes articles from a website using Selenium.\"\"\"\n",
    "    driver = init_driver()\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "\n",
    "    try:\n",
    "        driver.get(website)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "\n",
    "        #  Extract Articles from the Page\n",
    "        articles = extract_articles(driver)\n",
    "        print(f\" Extracted {len(articles)} articles before filtering.\")\n",
    "\n",
    "        #  Scroll if needed\n",
    "        if not any(keyword_pattern.search(title) for title, _ in articles):\n",
    "            print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "            articles = scroll_and_extract(driver)\n",
    "\n",
    "        #  Debug: Print All Extracted Articles Before Filtering\n",
    "        print(\"\\n DEBUG: Extracted Articles (Before Filtering):\")\n",
    "        for title, url in articles[:10]:  # Show first 10 for debugging\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "        #  Filter relevant articles using regex\n",
    "        filtered_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "\n",
    "        print(f\"\\n Found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "        for title, url in filtered_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "        #  Save unique results\n",
    "        for title, url in filtered_articles:\n",
    "            unique_articles.add((title, url))\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "#  Run scraping in parallel using ThreadPoolExecutor\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_website, websites)\n",
    "\n",
    "    #  Final Output: Only Relevant Articles\n",
    "    print(\"\\n SUMMARY - All Relevant Articles Found:\")\n",
    "    if unique_articles:\n",
    "        for title, url in unique_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "    else:\n",
    "        print(\" No relevant articles found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992049f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Regex for Multi-Word Matching (Looser matching for better results)\n",
    "keyword_pattern = re.compile(r'(?:' + '|'.join(re.escape(k) for k in keywords) + r')', re.IGNORECASE)\n",
    "\n",
    "#  Set to store unique articles\n",
    "unique_articles = set()\n",
    "\n",
    "#  Function to initialize Selenium WebDriver\n",
    "def init_driver():\n",
    "    driver = webdriver.Safari()\n",
    "    driver.set_page_load_timeout(30)  # Avoid hanging\n",
    "    return driver\n",
    "\n",
    "#  Function to extract article titles and links\n",
    "def extract_articles(driver):\n",
    "    \"\"\"Extracts article headlines and links with improved coverage.\"\"\"\n",
    "    extracted = []\n",
    "    retries = 3  # Retry extraction in case of stale elements\n",
    "    \n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "            #  Expanded JavaScript Query for More Complete Extraction\n",
    "            headlines = driver.execute_script(\"\"\"\n",
    "                return Array.from(document.querySelectorAll(\n",
    "                    'h1, h2, h3, h4, h5, h6, span, div, p, a, article, section'\n",
    "                ))\n",
    "                .map(el => ({ text: el.innerText.trim(), href: el.closest('a') ? el.closest('a').href : null }))\n",
    "                .filter(el => el.text && el.href);\n",
    "            \"\"\")\n",
    "\n",
    "            for item in headlines:\n",
    "                title, url = item[\"text\"], item[\"href\"]\n",
    "                if title and url:\n",
    "                    extracted.append((title, url))\n",
    "\n",
    "            break  # If successful, exit retry loop\n",
    "\n",
    "        except (StaleElementReferenceException, TimeoutException):\n",
    "            print(f\" Retrying extraction... ({attempt+1}/{retries})\")\n",
    "            time.sleep(2)\n",
    "\n",
    "    return extracted\n",
    "\n",
    "#  Function to scroll and extract more articles\n",
    "def scroll_and_extract(driver):\n",
    "    \"\"\"Scrolls the page multiple times to load more articles.\"\"\"\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(3)  # Allow loading time\n",
    "\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "\n",
    "        last_height = new_height\n",
    "\n",
    "    return extract_articles(driver)\n",
    "\n",
    "#  Function to scrape a website\n",
    "def scrape_website(website):\n",
    "    \"\"\"Scrapes articles from a website using Selenium.\"\"\"\n",
    "    driver = init_driver()\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "\n",
    "    try:\n",
    "        driver.get(website)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "\n",
    "        #  Extract Articles from the Page\n",
    "        articles = extract_articles(driver)\n",
    "        print(f\" Extracted {len(articles)} articles before filtering.\")\n",
    "\n",
    "        #  Debug: Print All Extracted Articles Before Filtering\n",
    "        print(\"\\n DEBUG: Extracted Articles (Before Filtering):\")\n",
    "        if articles:\n",
    "            for title, url in articles[:10]:  # Show first 10 for debugging\n",
    "                print(f\"{title} -> {url}\")\n",
    "        else:\n",
    "            print(\" No articles extracted from this website.\")\n",
    "\n",
    "        #  Scroll if needed\n",
    "        if not any(keyword_pattern.search(title) for title, _ in articles):\n",
    "            print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "            articles = scroll_and_extract(driver)\n",
    "\n",
    "        #  Debug: Check which keywords matched\n",
    "        matched_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "        print(f\"\\n DEBUG: Total Matched Articles Before Filtering: {len(matched_articles)}\")\n",
    "\n",
    "        if matched_articles:\n",
    "            for title, url in matched_articles[:10]:  # Show first 10 for debugging\n",
    "                print(f\" Matched: {title} -> {url}\")\n",
    "        else:\n",
    "            print(\" No articles matched the keywords.\")\n",
    "\n",
    "        #  Save unique results\n",
    "        for title, url in matched_articles:\n",
    "            unique_articles.add((title, url))\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "#  Run scraping in parallel using ThreadPoolExecutor\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_website, websites)\n",
    "\n",
    "    #  Final Output: Only Relevant Articles\n",
    "    print(\"\\n SUMMARY - All Relevant Articles Found:\")\n",
    "    if unique_articles:\n",
    "        for title, url in unique_articles:\n",
    "            print(f\"{title} -> {url}\")\n",
    "    else:\n",
    "        print(\" No relevant articles found.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d280340",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import re\n",
    "import time\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Compile regex for efficient keyword matching\n",
    "keyword_pattern = re.compile(r'(?:' + '|'.join(re.escape(k) for k in keywords) + r')', re.IGNORECASE)\n",
    "\n",
    "#  Set to store unique articles\n",
    "unique_articles = set()\n",
    "\n",
    "def init_driver():\n",
    "    driver = webdriver.Safari()\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver\n",
    "\n",
    "def extract_articles(driver):\n",
    "    \"\"\"Extracts article headlines and links with improved coverage.\"\"\"\n",
    "    extracted = []\n",
    "    try:\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "        #  Expanded JavaScript Query for More Complete Extraction\n",
    "        headlines = driver.execute_script(\"\"\"\n",
    "            return Array.from(document.querySelectorAll(\n",
    "                'h1, h2, h3, h4, span, div, p, a, article, section'\n",
    "            ))\n",
    "            .map(el => ({ text: el.innerText.trim(), href: el.closest('a') ? el.closest('a').href : null }))\n",
    "            .filter(el => el.text && el.href);\n",
    "        \"\"\")\n",
    "\n",
    "        for item in headlines:\n",
    "            title, url = item[\"text\"], item[\"href\"]\n",
    "            if title and url:\n",
    "                extracted.append((title, url))\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(\" Timeout while extracting articles.\")\n",
    "\n",
    "    return extracted\n",
    "\n",
    "def scrape_website(website):\n",
    "    \"\"\"Scrapes a website using Selenium.\"\"\"\n",
    "    driver = init_driver()\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "\n",
    "    try:\n",
    "        driver.get(website)\n",
    "        WebDriverWait(driver, 15).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "\n",
    "        articles = extract_articles(driver)\n",
    "\n",
    "        #  Debug: Print ALL Extracted Articles\n",
    "        print(f\"\\n DEBUG: Extracted Articles from {website}: {len(articles)}\")\n",
    "        for title, url in articles[:10]:  \n",
    "            print(f\"{title} -> {url}\")\n",
    "\n",
    "        #  Debug: Check Keyword Matches\n",
    "        matched_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "        print(f\"\\n DEBUG: Total Matched Articles on {website}: {len(matched_articles)}\")\n",
    "\n",
    "        for title, url in matched_articles[:10]:  \n",
    "            print(f\" Matched: {title} -> {url}\")\n",
    "\n",
    "        for title, url in matched_articles:\n",
    "            unique_articles.add((title, url))\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "def main():\n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        executor.map(scrape_website, websites)\n",
    "\n",
    "    print(\"\\n SUMMARY - All Relevant Articles Found:\")\n",
    "    for title, url in unique_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1c570e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import re\n",
    "\n",
    "#  STEP 1: Launch WebDriver and Open Website\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Browser launched and website loaded successfully!\")\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Compile regex for efficient keyword matching\n",
    "keyword_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(k) for k in keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs (with better error handling)\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links from the page with error handling.\"\"\"\n",
    "    extracted = []\n",
    "    \n",
    "    # Extract elements containing article headlines\n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div | //p | //a\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            # Try to find the closest <a> tag\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  # No <a> parent found, continue\n",
    "\n",
    "        # If URL is missing, check if it's an <a> tag itself\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        if title and url:\n",
    "            extracted.append((title, url))\n",
    "\n",
    "    return extracted\n",
    "\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "#  Debug: Show Extracted Headlines\n",
    "print(\"\\n DEBUG - Extracted Headlines & URLs:\\n\")\n",
    "for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "    print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "filtered_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  STEP 4: If No Relevant Articles, Scroll & Try Again\n",
    "if not filtered_articles:\n",
    "    print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Allow time for loading\n",
    "\n",
    "        new_articles = extract_articles()\n",
    "\n",
    "        #  Debugging: Print number of newly extracted articles\n",
    "        print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "        # Stop scrolling if no new content is loading\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    #  Re-run filtering after scrolling\n",
    "    articles = extract_articles()\n",
    "\n",
    "    print(\"\\nDEBUG AFTER SCROLLING - Extracted Headlines & URLs:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    filtered_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "    \n",
    "    print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close browser\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde7087e",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import re\n",
    "\n",
    "#  STEP 1: Launch Safari WebDriver and Open Website\n",
    "driver = webdriver.Safari()\n",
    "driver.get(\"https://www.upstreamonline.com\")\n",
    "\n",
    "#  Wait for page to fully load\n",
    "WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "print(\"\\n Safari WebDriver launched and website loaded successfully!\")\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Compile regex for efficient keyword matching\n",
    "keyword_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(k) for k in keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "\n",
    "#  STEP 2: Extract Headlines & URLs (with better error handling)\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links from the page with error handling.\"\"\"\n",
    "    extracted = []\n",
    "    \n",
    "    # Extract elements containing article headlines\n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div | //p | //a\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            # Try to find the closest <a> tag\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  # No <a> parent found, continue\n",
    "\n",
    "        # If URL is missing, check if it's an <a> tag itself\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        if title and url:\n",
    "            extracted.append((title, url))\n",
    "\n",
    "    return extracted\n",
    "\n",
    "\n",
    "articles = extract_articles()\n",
    "\n",
    "#  Debug: Show Extracted Headlines\n",
    "print(\"\\n DEBUG - Extracted Headlines & URLs:\\n\")\n",
    "for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "    print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "#  STEP 3: Filter Relevant Articles\n",
    "filtered_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "\n",
    "print(f\"\\n Found {len(filtered_articles)} relevant articles:\")\n",
    "for title, url in filtered_articles:\n",
    "    print(f\"{title} -> {url}\")\n",
    "\n",
    "#  STEP 4: If No Relevant Articles, Scroll & Try Again\n",
    "if not filtered_articles:\n",
    "    print(\"\\n No relevant articles found. Scrolling and rechecking...\\n\")\n",
    "\n",
    "    last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "    while True:\n",
    "        driver.find_element(By.TAG_NAME, \"body\").send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)  # Allow time for loading\n",
    "\n",
    "        new_articles = extract_articles()\n",
    "\n",
    "        #  Debugging: Print number of newly extracted articles\n",
    "        print(f\" Extracted {len(new_articles)} new articles after scrolling.\")\n",
    "\n",
    "        # Stop scrolling if no new content is loading\n",
    "        new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "        if new_height == last_height:\n",
    "            break\n",
    "        last_height = new_height\n",
    "\n",
    "    #  Re-run filtering after scrolling\n",
    "    articles = extract_articles()\n",
    "\n",
    "    print(\"\\nDEBUG AFTER SCROLLING - Extracted Headlines & URLs:\\n\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10 after scrolling\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    filtered_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "    \n",
    "    print(f\"\\n After scrolling, found {len(filtered_articles)} relevant articles:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "#  Close browser\n",
    "driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e343f20",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "import time\n",
    "import re\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Compile regex for efficient keyword matching\n",
    "keyword_pattern = re.compile(r'\\b(?:' + '|'.join(re.escape(k) for k in keywords) + r')\\b', re.IGNORECASE)\n",
    "\n",
    "#  Launch Safari WebDriver\n",
    "driver = webdriver.Safari()\n",
    "all_filtered_articles = []  # Store results from all websites\n",
    "\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with improved handling.\"\"\"\n",
    "    extracted = []\n",
    "    \n",
    "    try:\n",
    "        #  Wait for elements to load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "\n",
    "        #  Extract articles using JavaScript\n",
    "        headlines = driver.execute_script(\"\"\"\n",
    "            return Array.from(document.querySelectorAll('h1, h2, h3, h4, span, div, p, a'))\n",
    "                .map(el => ({\n",
    "                    text: el.innerText.trim(),\n",
    "                    href: el.closest('a') ? el.closest('a').href : null\n",
    "                }))\n",
    "                .filter(el => el.text && el.href);\n",
    "        \"\"\")\n",
    "\n",
    "        for item in headlines:\n",
    "            title, url = item[\"text\"], item[\"href\"]\n",
    "            if title and url:\n",
    "                extracted.append((title, url))\n",
    "\n",
    "    except (StaleElementReferenceException, TimeoutException):\n",
    "        print(\" Retrying extraction...\")\n",
    "        return extract_articles()\n",
    "\n",
    "    return extracted\n",
    "\n",
    "for website in websites:\n",
    "    print(f\"\\n Scraping Website: {website}\")\n",
    "    driver.get(website)\n",
    "\n",
    "    #  Wait for page to load\n",
    "    try:\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        print(f\" Website loaded successfully: {website}\")\n",
    "    except TimeoutException:\n",
    "        print(f\" Failed to load: {website}\")\n",
    "        continue\n",
    "\n",
    "    #  Extract articles\n",
    "    articles = extract_articles()\n",
    "\n",
    "    #  Debug: Show extracted headlines\n",
    "    print(f\"\\n DEBUG - Extracted Headlines from {website}:\")\n",
    "    for i, (title, url) in enumerate(articles[:10]):  # Show first 10\n",
    "        print(f\"{i+1}. {title} -> {url}\")\n",
    "\n",
    "    #  Debug: Check keyword matching before filtering\n",
    "    print(\"\\n DEBUG: Checking Keyword Matches BEFORE Filtering:\")\n",
    "    for title, url in articles:\n",
    "        if keyword_pattern.search(title):\n",
    "            print(f\" Matched: {title} -> {url}\")\n",
    "\n",
    "    #  Filter relevant articles using regex\n",
    "    filtered_articles = [(title, url) for title, url in articles if keyword_pattern.search(title)]\n",
    "\n",
    "    print(f\"\\n Found {len(filtered_articles)} relevant articles on {website}:\")\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "\n",
    "    #  Save results\n",
    "    all_filtered_articles.extend(filtered_articles)\n",
    "\n",
    "#  Close browser\n",
    "driver.quit()\n",
    "\n",
    "#  Final Output\n",
    "print(\"\\n SUMMARY - Total Relevant Articles Found Across All Websites:\")\n",
    "if all_filtered_articles:\n",
    "    for title, url in all_filtered_articles:\n",
    "        print(f\"{title} -> {url}\")\n",
    "else:\n",
    "    print(\" No relevant articles found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e9d8b1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "#  FIX: Disable Hugging Face tokenizer parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  STEP 1: Initialize WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with improved filtering.\"\"\"\n",
    "    extracted = set()\n",
    "    \n",
    "    headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "    for headline in headlines:\n",
    "        title = headline.text.strip()\n",
    "        url = None\n",
    "\n",
    "        try:\n",
    "            parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "            url = parent.get_attribute(\"href\")\n",
    "        except:\n",
    "            pass  \n",
    "\n",
    "        if not url and headline.tag_name == \"a\":\n",
    "            url = headline.get_attribute(\"href\")\n",
    "\n",
    "        if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "            extracted.add((title, url)) \n",
    "\n",
    "    return list(extracted)\n",
    "\n",
    "#  STEP 2: Scrape Each Website\n",
    "all_filtered_articles = []\n",
    "\n",
    "for website in websites:\n",
    "    try:\n",
    "        print(f\"\\n Scraping {website}...\")\n",
    "        driver.get(website)\n",
    "        \n",
    "        #  Wait for page to fully load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        \n",
    "        articles = extract_articles()\n",
    "\n",
    "        #  STEP 3: Filter Relevant Articles\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k.lower() in title.lower() for k in keywords)]\n",
    "        \n",
    "        print(f\" Found {len(filtered_articles)} relevant articles on {website}\")\n",
    "        all_filtered_articles.extend(filtered_articles)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error scraping {website}: {e}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "#  Display Filtered Articles\n",
    "print(\"\\n Relevant Articles Across All Websites:\\n\")\n",
    "\n",
    "for title, url in all_filtered_articles:\n",
    "    print(f\" {title}\\n {url}\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a5555d",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "\n",
    "#  Disable Hugging Face tokenizer parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Initialize WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with retry logic for stale element errors.\"\"\"\n",
    "    extracted = set()\n",
    "    \n",
    "    retries = 3  # Retry extracting elements up to 3 times if stale element error occurs\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "            for headline in headlines:\n",
    "                title = headline.text.strip()\n",
    "                url = None\n",
    "\n",
    "                try:\n",
    "                    parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "                    url = parent.get_attribute(\"href\")\n",
    "                except:\n",
    "                    pass  \n",
    "\n",
    "                if not url and headline.tag_name == \"a\":\n",
    "                    url = headline.get_attribute(\"href\")\n",
    "\n",
    "                if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "                    extracted.add((title, url)) \n",
    "\n",
    "            return list(extracted)\n",
    "\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries - 1:\n",
    "                print(f\" Stale element detected. Retrying extraction... ({attempt+1}/{retries})\")\n",
    "                time.sleep(1)  # Wait before retrying\n",
    "            else:\n",
    "                print(\" Failed to extract due to persistent stale element issue.\")\n",
    "                return []\n",
    "\n",
    "#  STEP 2: Scrape Each Website\n",
    "all_filtered_articles = []\n",
    "\n",
    "for website in websites:\n",
    "    try:\n",
    "        print(f\"\\n Scraping {website}...\")\n",
    "        driver.get(website)\n",
    "        \n",
    "        #  Wait for page to fully load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        \n",
    "        articles = extract_articles()\n",
    "\n",
    "        #  STEP 3: Filter Relevant Articles\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k.lower() in title.lower() for k in keywords)]\n",
    "        \n",
    "        print(f\" Found {len(filtered_articles)} relevant articles on {website}\")\n",
    "        all_filtered_articles.extend(filtered_articles)\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Timeout error while loading {website}. Skipping...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error scraping {website}: {e}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "#  Display Filtered Articles\n",
    "print(\"\\n Relevant Articles Across All Websites:\\n\")\n",
    "\n",
    "for title, url in all_filtered_articles:\n",
    "    print(f\" {title}\\n {url}\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087b8357",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "\n",
    "#  Disable Hugging Face tokenizer parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Initialize WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with retry logic for stale element errors.\"\"\"\n",
    "    extracted = set()\n",
    "    \n",
    "    retries = 3  # Retry extracting elements up to 3 times if stale element error occurs\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "            for headline in headlines:\n",
    "                title = headline.text.strip()\n",
    "                url = None\n",
    "\n",
    "                try:\n",
    "                    parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "                    url = parent.get_attribute(\"href\")\n",
    "                except:\n",
    "                    pass  \n",
    "\n",
    "                if not url and headline.tag_name == \"a\":\n",
    "                    url = headline.get_attribute(\"href\")\n",
    "\n",
    "                if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "                    extracted.add((title, url)) \n",
    "\n",
    "            return list(extracted)\n",
    "\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries - 1:\n",
    "                print(f\" Stale element detected. Retrying extraction... ({attempt+1}/{retries})\")\n",
    "                time.sleep(1)  # Wait before retrying\n",
    "            else:\n",
    "                print(\" Failed to extract due to persistent stale element issue.\")\n",
    "                return []\n",
    "\n",
    "#  STEP 2: Scrape Each Website\n",
    "all_filtered_articles = []\n",
    "\n",
    "for website in websites:\n",
    "    try:\n",
    "        print(f\"\\n Scraping {website}...\")\n",
    "        driver.get(website)\n",
    "        \n",
    "        #  Wait for page to fully load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        \n",
    "        articles = extract_articles()\n",
    "\n",
    "        #  STEP 3: Filter Relevant Articles\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k.lower() in title.lower() for k in keywords)]\n",
    "        \n",
    "        print(f\" Found {len(filtered_articles)} relevant articles on {website}\")\n",
    "        \n",
    "        if filtered_articles:\n",
    "            print(\"\\n Relevant URLs:\\n\")\n",
    "            for title, url in filtered_articles:\n",
    "                print(f\" {title}\\n {url}\\n\" + \"-\"*80)\n",
    "\n",
    "        all_filtered_articles.extend(filtered_articles)\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Timeout error while loading {website}. Skipping...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error scraping {website}: {e}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "#  Display All Filtered Articles\n",
    "print(\"\\n Relevant Articles Across All Websites:\\n\")\n",
    "\n",
    "for title, url in all_filtered_articles:\n",
    "    print(f\" {title}\\n {url}\\n\" + \"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f33848",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException\n",
    "\n",
    "#  Disable Hugging Face tokenizer parallelism warnings\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#  List of Websites to Scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Full Keyword List\n",
    "keywords = [\n",
    "    \"FID\", \"LNG Train\", \"ADES\", \"Valaris\", \"Shell\", \"SeaDrill\", \"Transocean\",\n",
    "    \"Topside\", \"Module\", \"Boskalis\", \"Orsted\", \"Ocean Winds\", \"Floatover\",\n",
    "    \"Tennet\", \"Borr Drilling\", \"Noble Drilling\", \"Deme\", \"Heerema\", \"Van Oord\",\n",
    "    \"Braemar\", \"JSI Alliance\", \"BOA\", \"Semi-Submersible\", \"Roll Group\", \"Saipem\",\n",
    "    \"McDermott\", \"SubSea7\", \"Seaway 7\", \"GPO\", \"Woodside\", \"ADNOC\", \"Kiewit\",\n",
    "    \"Petrofac\", \"Seatrium\", \"Samsung Heavy Industries\", \"Total\", \"LNG\",\n",
    "    \"Canada LNG\", \"Fearnleys\", \"Technip\", \"Larsen & Toubro\", \"Chevron\",\n",
    "    \"Diamond Offshore\", \"AllSeas\", \"BP\", \"Karadeniz\", \"Floating Wind\",\n",
    "    \"Shelf Drilling\", \"Fluor\", \"Jan De Nul\", \"Exxon Mobil\", \"Project Cargo\",\n",
    "    \"Topside Decommissioning\", \"Pipelay Vessel\", \"Mobilization\",\n",
    "    \"Demobilization\", \"Hanwha Ocean\", \"Offshore\", \"EPCI\", \"COOEC\",\n",
    "    \"gas compression trains\", \"Wellhead Platform\", \"Offshore Platform\",\n",
    "    \"Dry Dock\", \"FEED\", \"Floatel\", \"Heavy Lift Vessel\"\n",
    "]\n",
    "\n",
    "#  Initialize WebDriver\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "\n",
    "def extract_articles():\n",
    "    \"\"\"Extracts article headlines & links with retry logic for stale element errors.\"\"\"\n",
    "    extracted = set()\n",
    "    \n",
    "    retries = 3  # Retry extracting elements up to 3 times if stale element error occurs\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            headlines = driver.find_elements(By.XPATH, \"//h1 | //h2 | //h3 | //h4 | //span | //div\")\n",
    "\n",
    "            for headline in headlines:\n",
    "                title = headline.text.strip()\n",
    "                url = None\n",
    "\n",
    "                try:\n",
    "                    parent = headline.find_element(By.XPATH, \"./ancestor::a\")\n",
    "                    url = parent.get_attribute(\"href\")\n",
    "                except:\n",
    "                    pass  \n",
    "\n",
    "                if not url and headline.tag_name == \"a\":\n",
    "                    url = headline.get_attribute(\"href\")\n",
    "\n",
    "                if url and \"subscription\" not in url and \"login\" not in url and title and \"javascript:void(0)\" not in url:\n",
    "                    extracted.add((title, url)) \n",
    "\n",
    "            return list(extracted)\n",
    "\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries - 1:\n",
    "                print(f\" Stale element detected. Retrying extraction... ({attempt+1}/{retries})\")\n",
    "                time.sleep(1)  # Wait before retrying\n",
    "            else:\n",
    "                print(\" Failed to extract due to persistent stale element issue.\")\n",
    "                return []\n",
    "\n",
    "#  STEP 2: Scrape Each Website\n",
    "all_filtered_articles = {}\n",
    "\n",
    "for website in websites:\n",
    "    try:\n",
    "        print(f\"\\n Scraping {website}...\")\n",
    "        driver.get(website)\n",
    "        \n",
    "        #  Wait for page to fully load\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        \n",
    "        articles = extract_articles()\n",
    "\n",
    "        #  STEP 3: Filter Relevant Articles\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k.lower() in title.lower() for k in keywords)]\n",
    "        \n",
    "        if filtered_articles:\n",
    "            print(f\" Found {len(filtered_articles)} relevant articles on {website}\\n\")\n",
    "            all_filtered_articles[website] = filtered_articles\n",
    "            \n",
    "            #  Print each article title with its link\n",
    "            for title, url in filtered_articles:\n",
    "                print(f\" {title}\\n {url}\\n\" + \"-\"*80)\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Timeout error while loading {website}. Skipping...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error scraping {website}: {e}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "#  Display Filtered Articles\n",
    "print(\"\\n Relevant Articles Across All Websites:\\n\")\n",
    "\n",
    "for website, articles in all_filtered_articles.items():\n",
    "    print(f\"\\n {website}\\n\" + \"=\"*80)\n",
    "    for title, url in articles:\n",
    "        print(f\" {title}\\n {url}\\n\" + \"-\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6188b1a1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time      #MAIN !\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "\n",
    "# Initialize Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# List of industry websites to scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "# Keywords to filter relevant articles\n",
    "keywords = [\"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\", \"Floating Wind\", \"Pipelay Vessel\"]\n",
    "\n",
    "# Function to extract articles\n",
    "\n",
    "def extract_articles():\n",
    "    extracted = set()\n",
    "    retries = 3  # Retry mechanism for stale elements\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            elements = driver.find_elements(By.XPATH, \"//a\")  # Extract all links\n",
    "            for elem in elements:\n",
    "                title = elem.text.strip()\n",
    "                url = elem.get_attribute(\"href\")\n",
    "                if url and title:\n",
    "                    extracted.add((title, url))\n",
    "            return list(extracted)\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "# Start scraping process\n",
    "all_filtered_articles = []\n",
    "\n",
    "for website in websites:\n",
    "    try:\n",
    "        print(f\"Scraping {website}...\")\n",
    "        driver.get(website)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)  # Allow JavaScript-heavy pages to load\n",
    "        articles = extract_articles()\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k.lower() in title.lower() for k in keywords)]\n",
    "        all_filtered_articles.extend(filtered_articles)\n",
    "        print(f\"Found {len(filtered_articles)} relevant articles on {website}\")\n",
    "    except TimeoutException:\n",
    "        print(f\"Timeout error loading {website}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {website}: {e}\")\n",
    "\n",
    "# Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Save results\n",
    "df = pd.DataFrame(all_filtered_articles, columns=[\"Title\", \"URL\"])\n",
    "df.to_csv(\"filtered_articles.csv\", index=False)\n",
    "\n",
    "print(\"Filtered URLs saved to filtered_articles.csv!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e2f79c",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install selenium beautifulsoup4 requests pandas transformers torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3d857",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from transformers import pipeline\n",
    "\n",
    "#  Initialize AI Summarization Model\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "#  Initialize Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "#  List of industry websites to scrape\n",
    "websites = [\n",
    "    \"https://www.upstreamonline.com\",\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Keywords to filter relevant articles\n",
    "keywords = [\"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\", \"Floating Wind\", \"Pipelay Vessel\"]\n",
    "\n",
    "#  Function to Extract Article Links\n",
    "def extract_articles():\n",
    "    extracted = set()\n",
    "    retries = 3  # Retry mechanism for stale elements\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            elements = driver.find_elements(By.XPATH, \"//a\")  # Extract all links\n",
    "            for elem in elements:\n",
    "                title = elem.text.strip()\n",
    "                url = elem.get_attribute(\"href\")\n",
    "                if url and title:\n",
    "                    extracted.add((title, url))\n",
    "            return list(extracted)\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "#  Function to Scrape Full Article Content Using Requests (Faster)\n",
    "def get_article_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "        #  Extract the main article content (Modify tag based on website)\n",
    "        article_body = soup.find(\"div\", class_=\"article-content\")  # Update class if needed\n",
    "\n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "            content = \" \".join([p.get_text() for p in paragraphs])\n",
    "            return content[:3000]  # Limit content length for efficiency\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\" Error extracting content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "#  Function to Scrape Full Article Content Using Selenium (For JavaScript-Rendered Pages)\n",
    "def get_article_content_selenium(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)  # Allow JavaScript to load content\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        #  Extract article text (Update based on website structure)\n",
    "        article_body = soup.find(\"div\", class_=\"article-content\")  # Update class if needed\n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "            content = \" \".join([p.get_text() for p in paragraphs])\n",
    "            return content[:3000]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" Error extracting content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "#  Function to Summarize Article Content\n",
    "def summarize_text(text):\n",
    "    if not text or len(text.split()) < 50:  # Skip short articles\n",
    "        return \"Summary not available\"\n",
    "\n",
    "    summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
    "    return summary[0]['summary_text']\n",
    "\n",
    "#  Start Scraping Process\n",
    "all_filtered_articles = []\n",
    "\n",
    "for website in websites:\n",
    "    try:\n",
    "        print(f\"\\n Scraping {website}...\")\n",
    "        driver.get(website)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "        time.sleep(3)  # Allow JavaScript-heavy pages to load\n",
    "        articles = extract_articles()\n",
    "\n",
    "        #  Filter articles based on keywords\n",
    "        filtered_articles = [(title, url) for title, url in articles if any(k.lower() in title.lower() for k in keywords)]\n",
    "        print(f\" Found {len(filtered_articles)} relevant articles on {website}\")\n",
    "\n",
    "        #  Extract full content & Summarize\n",
    "        for title, url in filtered_articles:\n",
    "            print(f\" Processing: {title}\")\n",
    "            \n",
    "            #  First try using Requests\n",
    "            article_text = get_article_content(url)\n",
    "            \n",
    "            #  If Requests fails, use Selenium\n",
    "            if not article_text:\n",
    "                article_text = get_article_content_selenium(url)\n",
    "\n",
    "            #  Generate Summary\n",
    "            summary = summarize_text(article_text)\n",
    "            all_filtered_articles.append((title, url, summary))\n",
    "\n",
    "    except TimeoutException:\n",
    "        print(f\" Timeout error loading {website}\")\n",
    "    except Exception as e:\n",
    "        print(f\" Error scraping {website}: {e}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "#  Save Results\n",
    "df = pd.DataFrame(all_filtered_articles, columns=[\"Title\", \"URL\", \"Summary\"])\n",
    "df.to_csv(\"filtered_articles_with_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\n Filtered Articles with Summaries saved to filtered_articles_with_summary.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb10d0",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "\n",
    "#  Choose a single website to scrape (change this to any site you want)\n",
    "website = \"https://www.offshore-energy.biz/\"\n",
    "\n",
    "#  Keywords to filter relevant articles\n",
    "keywords = [\n",
    "    \"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\", \n",
    "    \"Floating Wind\", \"Pipelay Vessel\"\n",
    "]\n",
    "\n",
    "#  Initialize Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "#  Function to extract articles\n",
    "def extract_articles():\n",
    "    extracted = set()\n",
    "    retries = 3  # Retry mechanism for stale elements\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            elements = driver.find_elements(By.XPATH, \"//a\")  # Extract all links\n",
    "            for elem in elements:\n",
    "                title = elem.text.strip()\n",
    "                url = elem.get_attribute(\"href\")\n",
    "                if url and title:\n",
    "                    extracted.add((title, url))\n",
    "            return list(extracted)\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "#  Scrape the single website\n",
    "filtered_articles = []\n",
    "try:\n",
    "    print(f\"Scraping {website}...\")\n",
    "    driver.get(website)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    time.sleep(3)  # Allow JavaScript-heavy pages to load\n",
    "    articles = extract_articles()\n",
    "\n",
    "    #  Filter articles based on all keywords\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k.lower() in title.lower() for k in keywords)]\n",
    "    \n",
    "    print(f\" Found {len(filtered_articles)} relevant articles on {website}\")\n",
    "\n",
    "    #  Print each matched article\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\" {title}\\n {url}\\n{'='*80}\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(f\" Timeout error loading {website}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error scraping {website}: {e}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "#  Save results to CSV\n",
    "if filtered_articles:\n",
    "    df = pd.DataFrame(filtered_articles, columns=[\"Title\", \"URL\"])\n",
    "    df.to_csv(\"filtered_articles_offshore_energy.csv\", index=False)\n",
    "    print(\"\\n Filtered Articles (Titles & URLs) saved to filtered_articles_offshore_energy.csv!\")\n",
    "else:\n",
    "    print(\"\\n No relevant articles found for the given keywords.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9143fe59",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "\n",
    "#  Choose a single website to scrape (change this to any site you want)\n",
    "website = \"https://maritime-executive.com/\"\n",
    "\n",
    "#  Keywords to filter relevant articles\n",
    "keywords = [\n",
    "    \"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\", \n",
    "    \"Floating Wind\", \"Pipelay Vessel\"\n",
    "]\n",
    "\n",
    "#  Initialize Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode for efficiency\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "#  Function to extract articles\n",
    "def extract_articles():\n",
    "    extracted = set()\n",
    "    retries = 3  # Retry mechanism for stale elements\n",
    "\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            elements = driver.find_elements(By.XPATH, \"//a\")  # Extract all links\n",
    "            for elem in elements:\n",
    "                title = elem.text.strip()\n",
    "                url = elem.get_attribute(\"href\")\n",
    "                if url and title:\n",
    "                    extracted.add((title, url))\n",
    "            return list(extracted)\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "#  Scrape the single website\n",
    "filtered_articles = []\n",
    "try:\n",
    "    print(f\"Scraping {website}...\")\n",
    "    driver.get(website)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    time.sleep(3)  # Allow JavaScript-heavy pages to load\n",
    "    articles = extract_articles()\n",
    "\n",
    "    #  Filter articles based on all keywords\n",
    "    filtered_articles = [(title, url) for title, url in articles if any(k.lower() in title.lower() for k in keywords)]\n",
    "    \n",
    "    print(f\" Found {len(filtered_articles)} relevant articles on {website}\")\n",
    "\n",
    "    #  Print each matched article\n",
    "    for title, url in filtered_articles:\n",
    "        print(f\" {title}\\n {url}\\n{'='*80}\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(f\" Timeout error loading {website}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error scraping {website}: {e}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "#  Save results to CSV\n",
    "if filtered_articles:\n",
    "    df = pd.DataFrame(filtered_articles, columns=[\"Title\", \"URL\"])\n",
    "    df.to_csv(\"filtered_articles_offshore_energy.csv\", index=False)\n",
    "    print(\"\\n Filtered Articles (Titles & URLs) saved to filtered_articles_offshore_energy.csv!\")\n",
    "else:\n",
    "    print(\"\\n No relevant articles found for the given keywords.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946826e9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install selenium beautifulsoup4 requests pandas transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663cd75",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from transformers import pipeline\n",
    "\n",
    "#  Set the website to scrape\n",
    "website = \"https://www.offshore-energy.biz/\"\n",
    "\n",
    "#  Keywords to filter relevant articles\n",
    "keywords = [\n",
    "    \"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\", \n",
    "    \"Floating Wind\", \"Pipelay Vessel\"\n",
    "]\n",
    "\n",
    "#  Initialize summarization pipeline using Hugging Face Transformers\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
    "\n",
    "#  Setup Selenium WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "#  Extract article links and titles\n",
    "def extract_articles():\n",
    "    extracted = set()\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            elements = driver.find_elements(By.XPATH, \"//a\")\n",
    "            for elem in elements:\n",
    "                title = elem.text.strip()\n",
    "                url = elem.get_attribute(\"href\")\n",
    "                if url and title and url.startswith(\"http\"):\n",
    "                    extracted.add((title, url))\n",
    "            return list(extracted)\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "#  Extract article content using requests\n",
    "def get_article_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        article_body = soup.find(\"div\", class_=\"article__body\") or soup.find(\"div\", class_=\"entry-content\")\n",
    "\n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "            content = \" \".join([p.get_text() for p in paragraphs])\n",
    "            return content[:3000]  # limit size for summarization\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\" Error extracting content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "#  Generate summary for extracted content\n",
    "def summarize_text(text):\n",
    "    if not text or len(text.split()) < 50:\n",
    "        return \"Summary not available.\"\n",
    "    try:\n",
    "        result = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
    "        return result[0]['summary_text']\n",
    "    except Exception as e:\n",
    "        print(f\" Summarization error: {e}\")\n",
    "        return \"Summary generation failed.\"\n",
    "\n",
    "#  Main scraping + summarization logic\n",
    "filtered_articles = []\n",
    "try:\n",
    "    print(f\"Scraping {website}...\")\n",
    "    driver.get(website)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    time.sleep(3)  # for JS content\n",
    "\n",
    "    articles = extract_articles()\n",
    "    matched = [(title, url) for title, url in articles if any(k.lower() in title.lower() for k in keywords)]\n",
    "    print(f\" Found {len(matched)} relevant articles on {website}\")\n",
    "\n",
    "    for title, url in matched:\n",
    "        print(f\" {title}\\n {url}\")\n",
    "        content = get_article_content(url)\n",
    "        summary = summarize_text(content)\n",
    "        filtered_articles.append((title, url, summary))\n",
    "        print(f\" Summary: {summary[:100]}...\\n{'='*80}\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(f\" Timeout error loading {website}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error scraping {website}: {e}\")\n",
    "\n",
    "#  Close WebDriver\n",
    "driver.quit()\n",
    "\n",
    "#  Save to CSV\n",
    "if filtered_articles:\n",
    "    df = pd.DataFrame(filtered_articles, columns=[\"Title\", \"URL\", \"Summary\"])\n",
    "    df.to_csv(\"filtered_articles_offshore_energy_summary.csv\", index=False)\n",
    "    print(\"\\n Filtered Articles with Summaries saved to filtered_articles_offshore_energy_summary.csv!\")\n",
    "else:\n",
    "    print(\"\\n No relevant articles found or summarized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7772c0b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, StaleElementReferenceException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from transformers import pipeline\n",
    "\n",
    "#  Website to scrape\n",
    "website = \"https://www.offshore-energy.biz/\"\n",
    "\n",
    "#  Keywords to filter relevant articles\n",
    "keywords = [\"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\", \"Floating Wind\", \"Pipelay Vessel\"]\n",
    "\n",
    "#  Initialize summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\", tokenizer=\"t5-small\")\n",
    "\n",
    "#  Initialize WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")\n",
    "options.add_argument(\"--no-sandbox\")\n",
    "options.add_argument(\"--disable-dev-shm-usage\")\n",
    "service = Service(ChromeDriverManager().install())\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "#  Extract article links with visible text\n",
    "def extract_articles():\n",
    "    extracted = set()\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            elements = driver.find_elements(By.XPATH, \"//a\")\n",
    "            for elem in elements:\n",
    "                title = elem.text.strip()\n",
    "                url = elem.get_attribute(\"href\")\n",
    "                if url and title and url.startswith(\"http\"):\n",
    "                    extracted.add((title, url))\n",
    "            return list(extracted)\n",
    "        except StaleElementReferenceException:\n",
    "            if attempt < retries - 1:\n",
    "                time.sleep(1)\n",
    "            else:\n",
    "                return []\n",
    "\n",
    "#  Extract main article content\n",
    "def get_article_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        response = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        article_body = soup.find(\"div\", class_=\"article__body\") or soup.find(\"div\", class_=\"entry-content\")\n",
    "        if article_body:\n",
    "            paragraphs = article_body.find_all(\"p\")\n",
    "            content = \" \".join([p.get_text() for p in paragraphs])\n",
    "            return content[:3000]\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\" Error extracting content from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "#  Generate summary\n",
    "def summarize_text(text):\n",
    "    if not text or len(text.split()) < 50:\n",
    "        return \"Summary not available.\"\n",
    "    try:\n",
    "        result = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
    "        return result[0]['summary_text']\n",
    "    except Exception as e:\n",
    "        print(f\" Summarization error: {e}\")\n",
    "        return \"Summary generation failed.\"\n",
    "\n",
    "#  Scrape & summarize\n",
    "filtered_articles = []\n",
    "try:\n",
    "    print(f\"Scraping {website}...\")\n",
    "    driver.get(website)\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.TAG_NAME, \"body\")))\n",
    "    time.sleep(3)\n",
    "\n",
    "    articles = extract_articles()\n",
    "    matched = [\n",
    "        (title, url) for title, url in articles\n",
    "        if any(k.lower() in title.lower() for k in keywords)\n",
    "        and url.startswith(\"https://www.offshore-energy.biz/\")\n",
    "        and not url.strip().rstrip(\"/\").endswith(\"biz\")\n",
    "        and \"events\" not in url and \"about\" not in url\n",
    "    ]\n",
    "\n",
    "    print(f\" Found {len(matched)} relevant articles on {website}\")\n",
    "\n",
    "    for title, url in matched:\n",
    "        print(f\" {title}\\n {url}\")\n",
    "        content = get_article_content(url)\n",
    "        summary = summarize_text(content)\n",
    "        filtered_articles.append((title, url, summary))\n",
    "        print(f\" Summary: {summary[:100]}...\\n{'='*80}\")\n",
    "\n",
    "except TimeoutException:\n",
    "    print(f\" Timeout loading {website}\")\n",
    "except Exception as e:\n",
    "    print(f\" Error scraping {website}: {e}\")\n",
    "\n",
    "#  Close browser\n",
    "driver.quit()\n",
    "\n",
    "#  Save to CSV\n",
    "if filtered_articles:\n",
    "    df = pd.DataFrame(filtered_articles, columns=[\"Title\", \"URL\", \"Summary\"])\n",
    "    df.to_csv(\"filtered_articles_offshore_energy_summary.csv\", index=False)\n",
    "    print(\"\\n Articles with summaries saved to filtered_articles_offshore_energy_summary.csv!\")\n",
    "else:\n",
    "    print(\"\\n No relevant articles found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f0df9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Initialize summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "# Define source websites\n",
    "urls_to_scrape = [\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.upstreamonline.com/\",\n",
    "    \"https://www.rechargenews.com/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "# Filter keywords\n",
    "keywords = [\"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\", \"Floating Wind\", \"Pipelay Vessel\"]\n",
    "\n",
    "# Extract all article links from homepage\n",
    "def extract_article_links(site_url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        res = requests.get(site_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "        article_links = set()\n",
    "        for link in links:\n",
    "            title = link.get_text(strip=True)\n",
    "            href = link.get(\"href\")\n",
    "            if not title or not href:\n",
    "                continue\n",
    "            full_url = urljoin(site_url, href)\n",
    "            if any(k.lower() in title.lower() for k in keywords):\n",
    "                article_links.add((title, full_url))\n",
    "        return list(article_links)\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading {site_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Extract main text from article\n",
    "def extract_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        # Try common content containers\n",
    "        for class_name in [\"article__body\", \"entry-content\", \"article-body\", \"post-content\", \"main-content\"]:\n",
    "            container = soup.find(\"div\", class_=class_name)\n",
    "            if container:\n",
    "                text = \" \".join(p.get_text() for p in container.find_all(\"p\"))\n",
    "                return text.strip()[:3000]\n",
    "\n",
    "        # Fallback: gather all <p> tags\n",
    "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
    "        return text.strip()[:3000] if len(text) > 100 else None\n",
    "    except Exception as e:\n",
    "        print(f\" Error extracting from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Summarize article text\n",
    "def summarize(text):\n",
    "    try:\n",
    "        if not text or len(text.split()) < 50:\n",
    "            return \"Summary not available.\"\n",
    "        summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
    "        return summary[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\" Summarization error: {e}\")\n",
    "        return \"Summary failed.\"\n",
    "\n",
    "# Final output\n",
    "all_results = []\n",
    "\n",
    "# Loop through each site\n",
    "for site in urls_to_scrape:\n",
    "    print(f\"\\n Scraping site: {site}\")\n",
    "    article_links = extract_article_links(site)\n",
    "    print(f\" Found {len(article_links)} relevant articles with keywords.\")\n",
    "\n",
    "    for title, url in article_links:\n",
    "        print(f\" Processing: {title}\")\n",
    "        content = extract_content(url)\n",
    "        summary = summarize(content)\n",
    "        all_results.append({\"Site\": site, \"Title\": title, \"URL\": url, \"Summary\": summary})\n",
    "        print(f\" Done: {summary[:80]}...\\n{'-'*60}\")\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(\"all_sites_summaries.csv\", index=False)\n",
    "print(\"\\n Saved results to all_sites_summaries.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b21bd",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Initialize summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "# List of news site homepages\n",
    "urls_to_scrape = [\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.upstreamonline.com/\",\n",
    "    \"https://www.rechargenews.com/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "# Keywords to filter\n",
    "keywords = [\"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\", \"Floating Wind\", \"Pipelay Vessel\"]\n",
    "\n",
    "# Words that usually mean the URL isn't an article\n",
    "skip_words = [\n",
    "    \"about\", \"privacy\", \"cookie\", \"contact\", \"events\", \"magazine\", \"tag\", \"topic\",\n",
    "    \"category\", \"terms\", \".pdf\", \"advertise\", \"media\", \"jobs\", \"newsletter\", \"feedback\"\n",
    "]\n",
    "\n",
    "# Common HTML classes where article content may appear\n",
    "article_classes = [\n",
    "    \"article__body\", \"entry-content\", \"article-body\", \"post-content\", \"main-content\",\n",
    "    \"td-post-content\", \"article-content\", \"single-content\", \"c-article-body\"\n",
    "]\n",
    "\n",
    "# Extract links with keywords in titles\n",
    "def extract_article_links(site_url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        res = requests.get(site_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "        article_links = set()\n",
    "        for link in links:\n",
    "            title = link.get_text(strip=True)\n",
    "            href = link.get(\"href\")\n",
    "            if not title or not href:\n",
    "                continue\n",
    "            if any(x in href.lower() for x in skip_words):\n",
    "                continue\n",
    "            full_url = urljoin(site_url, href)\n",
    "            if any(k.lower() in title.lower() for k in keywords):\n",
    "                article_links.add((title, full_url))\n",
    "        return list(article_links)\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading {site_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Extract article content from a given URL\n",
    "def extract_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        for class_name in article_classes:\n",
    "            container = soup.find(\"div\", class_=class_name)\n",
    "            if container:\n",
    "                text = \" \".join(p.get_text() for p in container.find_all(\"p\"))\n",
    "                if len(text) > 100:\n",
    "                    return text.strip()[:3000]\n",
    "\n",
    "        # fallback: all <p> tags\n",
    "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
    "        return text.strip()[:3000] if len(text) > 100 else None\n",
    "    except Exception as e:\n",
    "        print(f\" Error extracting from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate AI summary\n",
    "def summarize(text):\n",
    "    try:\n",
    "        if not text or len(text.split()) < 50:\n",
    "            return \"Summary not available.\"\n",
    "        summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
    "        return summary[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\" Summarization error: {e}\")\n",
    "        return \"Summary failed.\"\n",
    "\n",
    "# Main processing loop\n",
    "all_results = []\n",
    "\n",
    "for site in urls_to_scrape:\n",
    "    print(f\"\\n Scraping: {site}\")\n",
    "    articles = extract_article_links(site)\n",
    "    print(f\" Found {len(articles)} keyword-matching articles.\")\n",
    "\n",
    "    for title, url in articles:\n",
    "        print(f\" {title}\\n {url}\")\n",
    "        content = extract_content(url)\n",
    "        summary = summarize(content)\n",
    "        all_results.append({\n",
    "            \"Site\": site,\n",
    "            \"Title\": title,\n",
    "            \"URL\": url,\n",
    "            \"Summary\": summary\n",
    "        })\n",
    "        print(f\" Summary: {summary[:80]}...\\n{'-'*60}\")\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(\"all_sites_summaries2.csv\", index=False)\n",
    "print(\"\\n All results saved to all_sites_summaries2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f1f5f6",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Initialize summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "# List of news site homepages\n",
    "urls_to_scrape = [\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.upstreamonline.com/\",\n",
    "    \"https://www.rechargenews.com/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "# Keywords to filter\n",
    "keywords = [\"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\", \"Floating Wind\", \"Pipelay Vessel\"]\n",
    "\n",
    "# Words that usually mean the URL isn't an article\n",
    "skip_words = [\n",
    "    \"about\", \"privacy\", \"cookie\", \"contact\", \"events\", \"magazine\", \"tag\", \"topic\",\n",
    "    \"category\", \"terms\", \".pdf\", \"advertise\", \"media\", \"jobs\", \"newsletter\", \"feedback\"\n",
    "]\n",
    "\n",
    "# Common HTML classes where article content may appear\n",
    "article_classes = [\n",
    "    \"article__body\", \"entry-content\", \"article-body\", \"post-content\", \"main-content\",\n",
    "    \"td-post-content\", \"article-content\", \"single-content\", \"c-article-body\"\n",
    "]\n",
    "\n",
    "# Extract links with keywords in titles\n",
    "def extract_article_links(site_url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        res = requests.get(site_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "        article_links = set()\n",
    "        for link in links:\n",
    "            title = link.get_text(strip=True)\n",
    "            href = link.get(\"href\")\n",
    "            if not title or not href:\n",
    "                continue\n",
    "            if any(x in href.lower() for x in skip_words):\n",
    "                continue\n",
    "            if href.rstrip(\"/\").endswith(\".biz\"):\n",
    "                continue\n",
    "            full_url = urljoin(site_url, href)\n",
    "            if any(k.lower() in title.lower() for k in keywords):\n",
    "                article_links.add((title, full_url))\n",
    "        return list(article_links)\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading {site_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Extract article content from a given URL\n",
    "def extract_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        for class_name in article_classes:\n",
    "            container = soup.find(\"div\", class_=class_name)\n",
    "            if container:\n",
    "                text = \" \".join(p.get_text() for p in container.find_all(\"p\"))\n",
    "                if len(text) > 100:\n",
    "                    return text.strip()[:3000]\n",
    "\n",
    "        # fallback: all <p> tags\n",
    "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
    "        return text.strip()[:3000] if len(text) > 100 else None\n",
    "    except Exception as e:\n",
    "        print(f\" Error extracting from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate AI summary\n",
    "def summarize(text):\n",
    "    try:\n",
    "        if not text or len(text.split()) < 50:\n",
    "            return \"Summary not available.\"\n",
    "        summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
    "        return summary[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\" Summarization error: {e}\")\n",
    "        return \"Summary failed.\"\n",
    "\n",
    "# Main processing loop\n",
    "all_results = []\n",
    "\n",
    "for site in urls_to_scrape:\n",
    "    print(f\"\\n Scraping: {site}\")\n",
    "    articles = extract_article_links(site)\n",
    "    print(f\" Found {len(articles)} keyword-matching articles.\")\n",
    "\n",
    "    for title, url in articles:\n",
    "        print(f\" {title}\\n {url}\")\n",
    "        content = extract_content(url)\n",
    "        summary = summarize(content)\n",
    "        all_results.append({\n",
    "            \"Site\": site,\n",
    "            \"Title\": title,\n",
    "            \"URL\": url,\n",
    "            \"Summary\": summary\n",
    "        })\n",
    "        print(f\" Summary: {summary[:80]}...\\n{'-'*60}\")\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(\"all_sites_summaries3.csv\", index=False)\n",
    "print(\"\\n All results saved to all_sites_summaries3.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7552b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from transformers import pipeline\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "#  Initialize summarizer\n",
    "summarizer = pipeline(\"summarization\", model=\"t5-small\")\n",
    "\n",
    "#  News websites to scrape\n",
    "urls_to_scrape = [\n",
    "    \"https://www.offshorewind.biz/\",\n",
    "    \"https://www.upstreamonline.com/\",\n",
    "    \"https://www.rechargenews.com/\",\n",
    "    \"https://www.offshore-energy.biz/\",\n",
    "    \"https://gcaptain.com/\",\n",
    "    \"https://www.oedigital.com/\",\n",
    "    \"https://maritime-executive.com/\",\n",
    "    \"https://www.marinelink.com/\",\n",
    "    \"https://www.tradewindsnews.com/\"\n",
    "]\n",
    "\n",
    "#  Keywords to filter article titles\n",
    "keywords = [\n",
    "    \"FID\", \"LNG\", \"Offshore\", \"Drilling\", \"Shell\", \"Transocean\",\n",
    "    \"Floating Wind\", \"Pipelay Vessel\"\n",
    "]\n",
    "\n",
    "#  URL patterns to skip\n",
    "skip_words = [\n",
    "    \"about\", \"privacy\", \"cookie\", \"contact\", \"events\", \"magazine\", \"tag\", \"topic\",\n",
    "    \"category\", \"terms\", \".pdf\", \"advertise\", \"media\", \"jobs\", \"newsletter\", \"feedback\"\n",
    "]\n",
    "\n",
    "#  Common article content containers\n",
    "article_classes = [\n",
    "    \"article__body\", \"entry-content\", \"article-body\", \"post-content\", \"main-content\",\n",
    "    \"td-post-content\", \"article-content\", \"single-content\", \"c-article-body\"\n",
    "]\n",
    "\n",
    "#  Get the base domain of a URL\n",
    "def get_base_domain(url):\n",
    "    parsed = urlparse(url)\n",
    "    return f\"{parsed.scheme}://{parsed.netloc}\"\n",
    "\n",
    "#  Extract links with keyword-matching titles\n",
    "def extract_article_links(site_url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        res = requests.get(site_url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "        links = soup.find_all(\"a\")\n",
    "        article_links = set()\n",
    "\n",
    "        for link in links:\n",
    "            title = link.get_text(strip=True)\n",
    "            href = link.get(\"href\")\n",
    "            if not title or not href:\n",
    "                continue\n",
    "            if any(x in href.lower() for x in skip_words):\n",
    "                continue\n",
    "            if href.rstrip(\"/\").endswith(\".biz\"):\n",
    "                continue\n",
    "            full_url = urljoin(site_url, href)\n",
    "            if any(k.lower() in title.lower() for k in keywords):\n",
    "                article_links.add((title, full_url))\n",
    "        return list(article_links)\n",
    "    except Exception as e:\n",
    "        print(f\" Error loading {site_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "#  Extract article content from the page\n",
    "def extract_content(url):\n",
    "    try:\n",
    "        headers = {\"User-Agent\": \"Mozilla/5.0\"}\n",
    "        res = requests.get(url, headers=headers, timeout=10)\n",
    "        soup = BeautifulSoup(res.text, \"html.parser\")\n",
    "\n",
    "        for class_name in article_classes:\n",
    "            container = soup.find(\"div\", class_=class_name)\n",
    "            if container:\n",
    "                text = \" \".join(p.get_text() for p in container.find_all(\"p\"))\n",
    "                if len(text) > 100:\n",
    "                    return text.strip()[:3000]\n",
    "\n",
    "        # Fallback: all <p> tags\n",
    "        text = \" \".join(p.get_text() for p in soup.find_all(\"p\"))\n",
    "        return text.strip()[:3000] if len(text) > 100 else None\n",
    "    except Exception as e:\n",
    "        print(f\" Error extracting from {url}: {e}\")\n",
    "        return None\n",
    "\n",
    "#  Generate summary\n",
    "def summarize(text):\n",
    "    try:\n",
    "        if not text or len(text.split()) < 50:\n",
    "            return \"Summary not available.\"\n",
    "        summary = summarizer(text, max_length=100, min_length=30, do_sample=False)\n",
    "        return summary[0][\"summary_text\"]\n",
    "    except Exception as e:\n",
    "        print(f\" Summarization error: {e}\")\n",
    "        return \"Summary failed.\"\n",
    "\n",
    "#  Process all sites\n",
    "all_results = []\n",
    "\n",
    "for site in urls_to_scrape:\n",
    "    print(f\"\\n Scraping: {site}\")\n",
    "    articles = extract_article_links(site)\n",
    "    print(f\" Found {len(articles)} keyword-matching articles.\")\n",
    "\n",
    "    for title, url in articles:\n",
    "        print(f\" {title}\\n {url}\")\n",
    "        content = extract_content(url)\n",
    "        summary = summarize(content)\n",
    "        all_results.append({\n",
    "            \"Site\": get_base_domain(url),  #  TRUE origin domain\n",
    "            \"Title\": title,\n",
    "            \"URL\": url,\n",
    "            \"Summary\": summary\n",
    "        })\n",
    "        print(f\" Summary: {summary[:80]}...\\n{'-'*60}\")\n",
    "\n",
    "#  Save to CSV\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv(\"all_sites_summaries3.csv\", index=False)\n",
    "print(\"\\n All results saved to all_sites_summaries3.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 8.805747,
   "end_time": "2025-04-09T03:38:35.339411",
   "environment_variables": {},
   "exception": true,
   "input_path": "Untitled-2.ipynb",
   "output_path": "output_notebook.ipynb",
   "parameters": {},
   "start_time": "2025-04-09T03:38:26.533664",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}